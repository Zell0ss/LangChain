{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain\n",
    "## Introduction\n",
    "LangChain is an open-source framework for developing applications powered by large language models (LLMs). A core purpose is to act as the \"glue\" that connects various components, including LLMs, to build reliable applications. It provides a standard, consistent interface for interacting with different LLM providers.\n",
    "LangChain's core components are \n",
    "- chains for linking multiple LLMs or tools\n",
    "- agents for enabling LLMs to interact with external resources\n",
    "- messages for structuring conversation input and output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to LLMs via a chat model\n",
    "LangChain can work with OpenAI, Cohere, Bloom, Huggingface, DeepSeek, and others. This makes LangChain model-agnostic, allowing developers to choose the best model for their needs while benefiting from LangChain's architecture.\n",
    "\n",
    "- Official models: These are models that are officially supported by LangChain and/or model provider. You can find these models in the langchain-<provider> packages.\n",
    "- Community models: There are models that are mostly contributed and supported by the community. You can find these models in the langchain-community package.\n",
    "\n",
    "For our test we will use the langchain_openai, wich need in the environment a variable called OPENAI_API_KEY with your api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "dotenv.load_dotenv()\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct questioning\n",
    "Old school, valid only for testing purposes. Its done directly calling to the chat_model  using the `.invoke()` method to pass a query. This will return an AIMessage object (we will talk about message roles just below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blood pressure is the force exerted by circulating blood against the walls of the body's arteries, the major blood vessels in the body. It is one of the principal vital signs used to assess the health of an individual. Blood pressure is typically expressed in terms of two measurements: systolic and diastolic pressure.\n",
      "\n",
      "1. **Systolic Pressure**: This is the higher number and measures the pressure in the arteries when the heart beats and pumps blood. It indicates how much pressure your blood is exerting against your artery walls when the heart is contracting.\n",
      "\n",
      "2. **Diastolic Pressure**: This is the lower number and measures the pressure in the arteries when the heart is at rest between beats. It indicates how much pressure your blood is exerting against your artery walls while the heart is resting between beats.\n",
      "\n",
      "Blood pressure is measured in millimeters of mercury (mmHg) and is recorded with the systolic number first, followed by the diastolic number (e.g., 120/80 mmHg).\n",
      "\n",
      "Normal blood pressure is typically around 120/80 mmHg, but what is considered normal can vary based on age, health conditions, and other factors. High blood pressure, or hypertension, is a condition where the blood pressure in the arteries is persistently elevated, which can lead to health issues such as heart disease and stroke. Conversely, low blood pressure, or hypotension, can cause dizziness and fainting and may indicate underlying health problems. Regular monitoring and management of blood pressure are important for maintaining cardiovascular health.\n"
     ]
    }
   ],
   "source": [
    "response = chat_model.invoke(\"Whats blood pressure?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages and roles\n",
    "LLMs are typically accessed through a chat model interface that takes a list of messages as input and returns a message as output. \n",
    "\n",
    "Messages are the unit of communication, representing model input and output, and are typically associated with roles like \"system,\" \"human,\" or \"assistant\". LangChain provides its own unified message format (e.g., SystemMessage, HumanMessage, AIMessage) \n",
    "\n",
    "- System Message: Used to prime the behavior of the AI model and provide additional context. It instructs the model on how to behave or sets the tone. For example, it can tell the model to act as an expert on a specific topic or to only answer questions within a certain domain. Not all providers support a dedicated system message role, but LangChain attempts to adapt by including the content in a human message or using a separate API parameter if supported. In LangChain's unified format, this is represented by the SystemMessage class.\n",
    "\n",
    "- Human Message: Represents input from a user interacting with the model. This is typically in the form of text, but some chat models also support multimodal content like images or audio. LangChain automatically converts a simple string input into a HumanMessage object for convenience. This is represented by the HumanMessage class in LangChain.\n",
    "\n",
    "- Assistant Message: Represents a response from the model. This response can include text or a request for the model to invoke tools. Multimodal outputs are possible but still uncommon. In LangChain's format, this is represented by the AIMessage class. When streaming responses, partial messages are returned as AIMessageChunk objects, which can be aggregated into a single AIMessage.\n",
    "\n",
    "- Tool Message: Used to pass the results of executing a tool back to the model. After a model requests to use a tool (via the tool_calls attribute in an AIMessage), this message type contains the output from that tool's execution. It includes a tool_call_id field linking it back to the specific tool call request. This is represented by the ToolMessage class.\n",
    "\n",
    "- Function Message (Legacy): This is a legacy message type that corresponded to OpenAI's previous function-calling API. The ToolMessage should be used instead for the updated tool-calling API. Represented by the Legacy FunctionMessage class.\n",
    "\n",
    "Besides these roles and their content, messages can also contain other data like a unique identifier (id), an optional name to differentiate speakers, metadata (like token usage), and importantly, tool_calls. The tool_calls attribute within an AIMessage represents a request from the model to call one or more tools, distinct from the ToolMessage which contains the result of that call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Blood pressure is the force exerted by circulating blood against the walls of the body's arteries, the major blood vessels in the body. It is one of the principal vital signs used to assess the health of an individual. Blood pressure is typically expressed in terms of two measurements: systolic and diastolic pressure.\\n\\n1. **Systolic Pressure**: This is the higher number and represents the pressure in the arteries when the heart beats and fills them with blood.\\n\\n2. **Diastolic Pressure**: This is the lower number and represents the pressure in the arteries when the heart is at rest between beats.\\n\\nBlood pressure is measured in millimeters of mercury (mmHg) and is usually recorded with the systolic number first, followed by the diastolic number, such as 120/80 mmHg.\\n\\nNormal blood pressure is generally considered to be around 120/80 mmHg. High blood pressure, or hypertension, is a condition where the blood pressure in the arteries is persistently elevated, which can lead to health issues such as heart disease, stroke, and kidney problems. Conversely, low blood pressure, or hypotension, can cause dizziness and fainting and may indicate underlying health issues. Regular monitoring and management of blood pressure are important for maintaining cardiovascular health.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 256, 'prompt_tokens': 29, 'total_tokens': 285, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-Bb9ThLhoP1ZeqNiyrcuAnJAID9tX8', 'finish_reason': 'stop', 'logprobs': None}, id='run-53fe4b2a-bcbf-456b-aa0c-216944012200-0', usage_metadata={'input_tokens': 29, 'output_tokens': 256, 'total_tokens': 285, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = []\n",
    "environment = SystemMessage(\n",
    "        content=\"\"\"You're an assistant knowledgeable about healthcare. Only answer healthcare-related questions.\"\"\"\n",
    "    )\n",
    "question  = HumanMessage(content=\"What is blood pressure?\")\n",
    "messages.append(environment)\n",
    "messages.append(question)\n",
    "chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm here to help with healthcare-related questions. If you have any questions about health or medical topics, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 56, 'total_tokens': 81, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-Bb9TmjObqw7jiY4ZCoLZ3WRY795Vn', 'finish_reason': 'stop', 'logprobs': None}, id='run-ca309b6f-7741-4037-8a41-4868bc2dc6e4-0', usage_metadata={'input_tokens': 56, 'output_tokens': 25, 'total_tokens': 81, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question  = HumanMessage(content=\"How to change a tire?\")\n",
    "messages.append(environment)\n",
    "messages.append(question)\n",
    "chat_model.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "As seen, modern Chat Models require a list of messages with specific roles, a simple PromptTemplate (which formats a single string) is insufficient for constructing the full input sequence in a structured way. This is where ChatPromptTemplate comes in. It is a type of prompt template specifically designed to format a list of messages.\n",
    "### simple prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'question', 'topic']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template_str = \"\"\"You're an expert on {topic}. ...\n",
    "Here is an user review:\n",
    "{context}\n",
    "\n",
    "Answer the following question in less than 10 words\n",
    "{question}\"\"\"\n",
    "prompt_template = PromptTemplate.from_template(template_str)\n",
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Yes, it is a positive review.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 43, 'total_tokens': 51, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-Bb9ToLiOckl8us9EEKt4yAzDLThEg', 'finish_reason': 'stop', 'logprobs': None}, id='run-4065d707-013e-4fff-adab-4c7cc4f95052-0', usage_metadata={'input_tokens': 43, 'output_tokens': 8, 'total_tokens': 51, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same filler, different template\n",
    "filled_prompt = prompt_template.format(\n",
    "    topic=\"User feedback\", context=\"I love it here!\", question=\"Is this a positive review?\"\n",
    ")\n",
    "chat_model.invoke(filled_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### system msg, human msg and chat prompt templates\n",
    "We saw that there are different roles for the messages, tipically, the system and human messages. There are specific prompt templates that generate role colored messages:\n",
    "- SystemMessagePromptTemplate\n",
    "- HumanMessagePromptTemplate\n",
    "\n",
    "All the messages can be joined in a ChatPromptTemplate to replace all its variables and send it through the invoke method.\n",
    "\n",
    "This ready-to-be-filled PromptTemplate templates can be created with the `from_template()` function wich will locate and create the parameters of the template automatically\n",
    "```python\n",
    "review_system_prompt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate.from_template(review_system_template_str)\n",
    ")\n",
    "``` \n",
    "or invoking directly the creator:\n",
    "```python\n",
    "review_system_prompt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=review_template_str,\n",
    "    )\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Your job is to use patient\\nreviews to answer questions about their experience at a\\nhospital. Use the following context to answer questions.\\nBe as detailed as possible, but don't make up any information\\nthat's not from the context. If you don't know an answer, say\\nyou don't know.\\n\\nPatient reviews:\\n\\n{context}\\n\") additional_kwargs={}\n",
      "prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}') additional_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "\n",
    "review_system_template_str = \"\"\"Your job is to use patient\n",
    "reviews to answer questions about their experience at a\n",
    "hospital. Use the following context to answer questions.\n",
    "Be as detailed as possible, but don't make up any information\n",
    "that's not from the context. If you don't know an answer, say\n",
    "you don't know.\n",
    "\n",
    "Patient reviews:\n",
    "\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "review_system_prompt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate.from_template(review_system_template_str)\n",
    ")\n",
    "print (review_system_prompt)\n",
    "\n",
    "review_human_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate.from_template(\"{question}\")\n",
    ")\n",
    "print(review_human_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Your job is to use patient\\nreviews to answer questions about their experience at a\\nhospital. Use the following context to answer questions.\\nBe as detailed as possible, but don't make up any information\\nthat's not from the context. If you don't know an answer, say\\nyou don't know.\\n\\nPatient reviews:\\n\\n{context}\\n\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "messages = [review_system_prompt, review_human_prompt]\n",
    "review_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "review_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Yes, one patient mentioned having a great stay, which indicates a positive experience.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 86, 'total_tokens': 102, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_90122d973c', 'id': 'chatcmpl-Bb9ToO9xbNxmOdYaAT3cC6cU9XOkj', 'finish_reason': 'stop', 'logprobs': None} id='run-d78ff43c-378c-4884-9058-858417d6f3a2-0' usage_metadata={'input_tokens': 86, 'output_tokens': 16, 'total_tokens': 102, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "context = \"I had a great stay!\"\n",
    "question = \"Did anyone have a positive experience?\"\n",
    "\n",
    "filled_prompt = review_prompt_template.format_messages(\n",
    "    context=context, question=question\n",
    ")\n",
    "\n",
    "print(chat_model.invoke(filled_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains\n",
    "Chains are a core concept in LangChain, representing an end-to-end wrapper around multiple individual components executed in a defined order. They allow developers to go beyond a single API call to an LLM and instead chain together multiple calls in a logical sequence. This enables breaking down complex tasks into smaller steps, maintaining context between calls by feeding the output of one step as input to the next, and adding intermediate processing logic. \n",
    "\n",
    "LangChain offers various types of chains:\n",
    "\n",
    "- LLM Chain: The simplest form, consisting of a PromptTemplate, a language model (LLM or ChatModel), and an optional output parser. We can see here the steps and how they feed on the output of the previous one: \n",
    "    - takes the input parameters\n",
    "    - uses the PromptTemplate to format them into a prompt\n",
    "    - passes the prompt to the model\n",
    "    - optionally uses the OutputParser to refine the result.\n",
    "- Sequential Chain: Combines various individual chains where the output of one chain serves as the input for the next in a continuous sequence. There are two types: Simple Sequential Chains, which handle a single input and output, and a more general form that allows for multiple inputs/outputs.\n",
    "- Router Chain: Used for complicated tasks when there are multiple subchains specialized for different types of input. It consists of a Router Chain that selects the next chain, Destination Chains that it can route to, and a Default Chain used when the router cannot decide. This adds intelligent decision-making by directing inputs to the most suitable processing paths.\n",
    "- Transform Chain: Applies a data transformation between chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Yes, the patient had a positive experience, as indicated by their review stating, \"I had a great stay!\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 87, 'total_tokens': 110, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bb9TqxxpHyyFgOSHghpOkRi6Vz0oi', 'finish_reason': 'stop', 'logprobs': None} id='run-53aafbf5-3d08-486b-9c00-d35d83104870-0' usage_metadata={'input_tokens': 87, 'output_tokens': 23, 'total_tokens': 110, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content=\"I'm sorry, but the context provided does not include information about what the patient was interned for.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 86, 'total_tokens': 106, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_76544d79cb', 'id': 'chatcmpl-Bb9TrzVc6ytY4wEULQlhOpyjuAaYh', 'finish_reason': 'stop', 'logprobs': None} id='run-f8591e3c-4987-47eb-8d89-ba57194e11ad-0' usage_metadata={'input_tokens': 86, 'output_tokens': 20, 'total_tokens': 106, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content=\"I'm sorry, but the context provided does not contain information on how to fix a car. It only mentions a patient review about needing a car fixed due to a punctured tire.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 91, 'total_tokens': 127, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9bddfca6e2', 'id': 'chatcmpl-Bb9Tt8bARYckZIgx02H7egx0uyVKA', 'finish_reason': 'stop', 'logprobs': None} id='run-452ce401-b044-48d2-83e1-fdf11655b369-0' usage_metadata={'input_tokens': 91, 'output_tokens': 36, 'total_tokens': 127, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# define the chain\n",
    "review_chain = review_prompt_template | chat_model\n",
    "\n",
    "context = \"I had a great stay!\"\n",
    "question = \"Did the patient have a positive experience?\"\n",
    "# invoke it passing the parameters of the template for the first step\n",
    "print(review_chain.invoke({\"context\": context, \"question\": question}))\n",
    "\n",
    "question = \"what was he interned for?\"\n",
    "print(review_chain.invoke({\"context\": context, \"question\": question}))\n",
    "\n",
    "context = \"I need my car fixed. it has a punctured tire\"\n",
    "question = \"how to fix his car\"\n",
    "print(review_chain.invoke({\"context\": context, \"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting a chain\n",
    "For complex chains it will be good to have on your toolbelt the langchain_core.globals > set_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"context\": \"I had a great stay! I had terrible stomac ache for 3 days and a stone was found in my liver\",\n",
      "  \"question\": \"what was he interned for?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"context\": \"I had a great stay! I had terrible stomac ache for 3 days and a stone was found in my liver\",\n",
      "  \"question\": \"what was he interned for?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Your job is to use patient\\nreviews to answer questions about their experience at a\\nhospital. Use the following context to answer questions.\\nBe as detailed as possible, but don't make up any information\\nthat's not from the context. If you don't know an answer, say\\nyou don't know.\\n\\nPatient reviews:\\n\\nI had a great stay! I had terrible stomac ache for 3 days and a stone was found in my liver\\n\\nHuman: what was he interned for?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [802ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The patient was interned for a terrible stomach ache that lasted for 3 days, and a stone was found in their liver.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The patient was interned for a terrible stomach ache that lasted for 3 days, and a stone was found in their liver.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 26,\n",
      "                \"prompt_tokens\": 105,\n",
      "                \"total_tokens\": 131,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "              \"system_fingerprint\": \"fp_07871e2ad8\",\n",
      "              \"id\": \"chatcmpl-Bb9hDZbe0NDSJa9YeQXGDxHQ64a5C\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-673851b9-26ab-47db-97f8-ec92b039c841-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 105,\n",
      "              \"output_tokens\": 26,\n",
      "              \"total_tokens\": 131,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 26,\n",
      "      \"prompt_tokens\": 105,\n",
      "      \"total_tokens\": 131,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "    \"system_fingerprint\": \"fp_07871e2ad8\",\n",
      "    \"id\": \"chatcmpl-Bb9hDZbe0NDSJa9YeQXGDxHQ64a5C\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The patient was interned for a terrible stomach ache that lasted for 3 days, and a stone was found in their liver.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [811ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The patient was interned for a terrible stomach ache that lasted for 3 days, and a stone was found in their liver.\"\n",
      "}\n",
      "The patient was interned for a terrible stomach ache that lasted for 3 days, and a stone was found in their liver.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.globals import set_debug\n",
    "set_debug(True)\n",
    "\n",
    "print(review_chain.invoke({\"context\": context, \"question\": question}))\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output parser\n",
    "So far we have in our chain a ChatPromptTemplate (review_prompt_template) and an LLM, in our case a ChatOpenAI (chat_model): We can add to the end a third block, an output parser to ease the reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient described their stay as great.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# redefine the chain\n",
    "review_chain = review_prompt_template | chat_model | output_parser\n",
    "\n",
    "#fill the parameters and invoke\n",
    "context = \"I had a great stay! I had terrible stomach pain. I ache for 3 days and a stone was found and removed from my liver\"\n",
    "question = \"How was his stay?\"\n",
    "response = review_chain.invoke({\"context\": context, \"question\": question})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of reviews I have access to is 24.\n",
      "\n",
      "Pure Positive Reviews:\n",
      "1. Review by Christy Johnson: \"The medical staff at the hospital were incredibly attentive and supportive during my stay. The facilities were top-notch, making my recovery comfortable and smooth.\"\n",
      "2. Review by Anna Frazier: \"The hospital's commitment to patient education impressed me. The medical team took the time to explain my diagnosis and treatment options, empowering me to make informed decisions about my health.\"\n",
      "3. Review by Abigail Mitchell: \"The hospital's commitment to patient safety was evident throughout my stay. The stringent hygiene protocols and vigilant staff instilled confidence in the quality of care provided.\"\n",
      "4. Review by Cody Ibarra: \"I'm grateful for the compassionate care I received at the hospital. The medical team was thorough in their approach, and the facilities were clean and comfortable.\"\n",
      "5. Review by Scott Terry: \"My stay at the hospital was great. The nurses were friendly and efficient, and the doctors were knowledgeable and thorough in their examinations.\"\n",
      "\n",
      "Pure Negative Reviews:\n",
      "1. Review by Rachel Carter: \"My time at the hospital was disappointing. The facilities were outdated, and the lack of communication between the medical team and me left me feeling uninformed and frustrated.\"\n",
      "\n",
      "Mixed Reviews:\n",
      "1. Review by Kimberly Rivas: \"I had a positive experience overall at the hospital. The medical staff was caring and attentive, and the facilities were modern and well-equipped. However, the parking situation was inconvenient.\"\n",
      "2. Review by Catherine Yang: \"The medical team at the hospital was exceptional, and the facilities were state-of-the-art. The only downside was the noise level in the shared rooms, affecting my rest.\"\n",
      "3. Review by Jennifer Russell: \"While the medical care was excellent, the wait times for tests and results were quite frustrating. Improvement in this area would greatly enhance the overall experience.\"\n",
      "4. Review by Henry Hays: \"The medical team was attentive, and the facilities were clean. Unfortunately, the noise levels in the hallway were disruptive, affecting the overall peacefulness of the environment.\"\n",
      "5. Review by Kim Franklin: \"The hospital provided exceptional care, and the nursing staff was incredibly supportive. However, the administrative processes were a bit convoluted, causing some confusion.\"\n",
      "6. Review by Michael Smith: \"The hospital staff was friendly and supportive throughout my stay. The only drawback was the limited parking space.\"\n",
      "7. Review by Chelsea Mitchell: \"While the hospital had state-of-the-art equipment, the staff's lack of coordination resulted in delays and confusion regarding my treatment.\"\n",
      "8. Review by Carol Byrd: \"The hospital provided exceptional care, and the nursing staff was incredibly supportive. However, the administrative processes were a bit convoluted, causing some confusion.\"\n",
      "9. Review by Daniel Williams: \"I was pleased with the level of care I received at the hospital. The only downside was the confusing layout, making it easy to get lost in the corridors.\"\n",
      "10. Review by Kim Powers: \"The nursing staff was caring, and the hospital had a calming ambiance. The lack of vegetarian options in the cafeteria, however, was a disappointment.\"\n",
      "11. Review by Sharon Brown: \"I appreciate the hospital's commitment to patient safety, and the cleanliness standards were commendable. However, the lack of entertainment options for patients during recovery was a downside.\"\n",
      "12. Review by John Bartlett: \"I had a mixed experience at the hospital. The medical team was attentive, but the lack of communication about the potential risks of a procedure was concerning. The facilities, however, were modern and clean.\"\n",
      "13. Review by Rebecca Wilkerson: \"I had a positive experience with the hospital's medical team, who provided excellent care. Nevertheless, the administrative processes, especially the check-in and discharge, could be more streamlined.\"\n",
      "14. Review by Michele Jones: \"The hospital's facilities were modern and well-maintained. However, the lack of communication about changes in my treatment plan created unnecessary stress.\"\n",
      "15. Review by Tiffany Long: \"The hospital staff was caring and attentive. However, the lack of communication between different departments led to some confusion about my treatment plan.\"\n",
      "16. Review by Stacy Villa: \"I had a positive overall experience at the hospital. The facilities were modern, and the medical team was thorough. The only drawback was the noise from other patients in the shared room.\"\n",
      "\n",
      "In summary:\n",
      "- Total reviews: 24\n",
      "- Pure positive reviews: 5\n",
      "- Pure negative reviews: 1\n",
      "- Mixed reviews: 18\n"
     ]
    }
   ],
   "source": [
    "from reviews import reviews\n",
    "response = review_chain.invoke({\"context\": reviews, \"question\": \"tell me the total number of reviews, the number of pure positive ones, the number of puere negative ones and the number of mixed ones that you have access to ?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
