{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain\n",
    "## Introduction\n",
    "LangChain is an open-source framework for developing applications powered by large language models (LLMs). A core purpose is to act as the \"glue\" that connects various components, including LLMs, to build reliable applications. It provides a standard, consistent interface for interacting with different LLM providers.\n",
    "LangChain's core components are \n",
    "- chains for linking multiple LLMs or tools\n",
    "- agents for enabling LLMs to interact with external resources\n",
    "- messages, defined by its role and its content, for structuring conversation input and output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to LLMs via a chat model\n",
    "LangChain can work with OpenAI, Cohere, Bloom, Huggingface, DeepSeek, and others. This makes LangChain model-agnostic, allowing developers to choose the best model for their needs while benefiting from LangChain's architecture.\n",
    "\n",
    "- Official models: These are models that are officially supported by LangChain and/or model provider. You can find these models in the langchain-<provider> packages.\n",
    "- Community models: There are models that are mostly contributed and supported by the community. You can find these models in the langchain-community package.\n",
    "\n",
    "For our test we will use the langchain_openai, wich need in the environment a variable called OPENAI_API_KEY with your api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "dotenv.load_dotenv()\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct questioning\n",
    "Old school, valid only for testing purposes. Its done directly calling to the chat_model  using the `.invoke()` method to pass a query. This will return an AIMessage object (we will talk about message roles just below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blood pressure is the force exerted by circulating blood against the walls of the body's arteries, the major blood vessels in the body. It is one of the vital signs used to assess the overall health of an individual. Blood pressure is typically expressed in terms of two measurements: systolic and diastolic pressure.\n",
      "\n",
      "1. **Systolic Pressure**: This is the higher number and measures the pressure in the arteries when the heart beats and pumps blood.\n",
      "\n",
      "2. **Diastolic Pressure**: This is the lower number and measures the pressure in the arteries when the heart is at rest between beats.\n",
      "\n",
      "Blood pressure is usually recorded as two numbers, written as a ratio, such as 120/80 mmHg (millimeters of mercury). The first number is the systolic pressure, and the second is the diastolic pressure.\n",
      "\n",
      "Normal blood pressure for most adults is typically around 120/80 mmHg. However, what is considered normal can vary based on age, health conditions, and other factors. High blood pressure, or hypertension, is a condition where the blood pressure is consistently too high, which can lead to health problems such as heart disease and stroke. Low blood pressure, or hypotension, can also be a concern if it causes symptoms like dizziness or fainting. Regular monitoring and management of blood pressure are important for maintaining cardiovascular health.\n"
     ]
    }
   ],
   "source": [
    "response = chat_model.invoke(\"Whats blood pressure?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages and roles\n",
    "LLMs are typically accessed through a chat model interface that takes a list of messages as input and returns a message as output. \n",
    "\n",
    "Messages are the unit of communication, representing model input and output, and are typically associated with roles like \"system,\" \"human,\" or \"assistant\". LangChain provides its own unified message format (e.g., SystemMessage, HumanMessage, AIMessage) \n",
    "\n",
    "- System Message: Used to prime the behavior of the AI model and provide additional context. It instructs the model on how to behave or sets the tone. For example, it can tell the model to act as an expert on a specific topic or to only answer questions within a certain domain. Not all providers support a dedicated system message role, but LangChain attempts to adapt by including the content in a human message or using a separate API parameter if supported. In LangChain's unified format, this is represented by the SystemMessage class.\n",
    "\n",
    "- Human Message: Represents input from a user interacting with the model. This is typically in the form of text, but some chat models also support multimodal content like images or audio. LangChain automatically converts a simple string input into a HumanMessage object for convenience. This is represented by the HumanMessage class in LangChain.\n",
    "\n",
    "- Assistant Message: Represents a response from the model. This response can include text or a request for the model to invoke tools. Multimodal outputs are possible but still uncommon. In LangChain's format, this is represented by the AIMessage class. When streaming responses, partial messages are returned as AIMessageChunk objects, which can be aggregated into a single AIMessage.\n",
    "\n",
    "- Tool Message: Used to pass the results of executing a tool back to the model. After a model requests to use a tool (via the tool_calls attribute in an AIMessage), this message type contains the output from that tool's execution. It includes a tool_call_id field linking it back to the specific tool call request. This is represented by the ToolMessage class.\n",
    "\n",
    "- Function Message (Legacy): This is a legacy message type that corresponded to OpenAI's previous function-calling API. The ToolMessage should be used instead for the updated tool-calling API. Represented by the Legacy FunctionMessage class.\n",
    "\n",
    "Besides these roles and their content, messages can also contain other data like a unique identifier (id), an optional name to differentiate speakers, metadata (like token usage), and importantly, tool_calls. The tool_calls attribute within an AIMessage represents a request from the model to call one or more tools, distinct from the ToolMessage which contains the result of that call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Blood pressure is the force exerted by circulating blood against the walls of the body's arteries, the major blood vessels in the body. It is one of the principal vital signs used to assess the health of an individual. Blood pressure is typically expressed in terms of two measurements: systolic and diastolic pressure.\\n\\n1. **Systolic Pressure**: This is the higher number and represents the pressure in the arteries when the heart beats and fills them with blood.\\n\\n2. **Diastolic Pressure**: This is the lower number and represents the pressure in the arteries when the heart is at rest between beats.\\n\\nBlood pressure is measured in millimeters of mercury (mmHg) and is recorded with the systolic number first, followed by the diastolic number, for example, 120/80 mmHg.\\n\\nNormal blood pressure is generally considered to be around 120/80 mmHg. High blood pressure, or hypertension, is a condition where the blood pressure is consistently too high, which can lead to health problems such as heart disease and stroke. Low blood pressure, or hypotension, can also be a concern if it causes symptoms like dizziness or fainting. Regular monitoring and management of blood pressure are important for maintaining cardiovascular health.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 249, 'prompt_tokens': 29, 'total_tokens': 278, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9bddfca6e2', 'id': 'chatcmpl-BbuACF7qJ11MW7qAU6hKAhBrHoxD6', 'finish_reason': 'stop', 'logprobs': None}, id='run-6d87359e-6a32-4171-b3d3-e4bc3968b594-0', usage_metadata={'input_tokens': 29, 'output_tokens': 249, 'total_tokens': 278, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = []\n",
    "environment = SystemMessage(\n",
    "        content=\"\"\"You're an assistant knowledgeable about healthcare. Only answer healthcare-related questions.\"\"\"\n",
    "    )\n",
    "question  = HumanMessage(content=\"What is blood pressure?\")\n",
    "messages.append(environment)\n",
    "messages.append(question)\n",
    "chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm here to help with healthcare-related questions. If you have any questions about health or medical topics, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 56, 'total_tokens': 81, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_76544d79cb', 'id': 'chatcmpl-BbuAIupkW2zR6WsW6ZVzAZMTraV2N', 'finish_reason': 'stop', 'logprobs': None}, id='run-9ceb8e1b-6925-4183-86a1-e2a6740aa006-0', usage_metadata={'input_tokens': 56, 'output_tokens': 25, 'total_tokens': 81, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question  = HumanMessage(content=\"How to change a tire?\")\n",
    "messages.append(environment)\n",
    "messages.append(question)\n",
    "chat_model.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "As seen, modern Chat Models require a list of messages with specific roles, a simple PromptTemplate (which formats a single string) is insufficient for constructing the full input sequence in a structured way. This is where ChatPromptTemplate comes in. It is a type of prompt template specifically designed to format a list of messages.\n",
    "### simple prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'question', 'topic']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template_str = \"\"\"You're an expert on {topic}. ...\n",
    "Here is an user review:\n",
    "{context}\n",
    "\n",
    "Answer the following question in less than 10 words\n",
    "{question}\"\"\"\n",
    "prompt_template = PromptTemplate.from_template(template_str)\n",
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Yes, it is a positive review.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 43, 'total_tokens': 51, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_76544d79cb', 'id': 'chatcmpl-BbuATUzeDsHP8qUX37VcYaYHZJr6b', 'finish_reason': 'stop', 'logprobs': None}, id='run-32375033-249e-4fbd-be90-014a304d3efb-0', usage_metadata={'input_tokens': 43, 'output_tokens': 8, 'total_tokens': 51, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same filler, different template\n",
    "filled_prompt = prompt_template.format(\n",
    "    topic=\"User feedback\", context=\"I love it here!\", question=\"Is this a positive review?\"\n",
    ")\n",
    "chat_model.invoke(filled_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### system msg, human msg and chat prompt templates\n",
    "We saw that there are different roles for the messages, tipically, the system and human messages. There are specific prompt templates that generate role colored messages:\n",
    "- SystemMessagePromptTemplate\n",
    "- HumanMessagePromptTemplate\n",
    "\n",
    "All the messages can be joined in a ChatPromptTemplate to replace all its variables and send it through the invoke method.\n",
    "\n",
    "This ready-to-be-filled PromptTemplate templates can be created with the `from_template()` function wich will locate and create the parameters of the template automatically\n",
    "```python\n",
    "review_system_prompt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate.from_template(review_system_template_str)\n",
    ")\n",
    "``` \n",
    "or invoking directly the creator:\n",
    "```python\n",
    "review_system_prompt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=review_template_str,\n",
    "    )\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Your job is to use patient\\nreviews to answer questions about their experience at a\\nhospital. Use the following context to answer questions.\\nBe as detailed as possible, but don't make up any information\\nthat's not from the context. If you don't know an answer, say\\nyou don't know.\\n\\nPatient reviews:\\n\\n{context}\\n\") additional_kwargs={}\n",
      "prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}') additional_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "\n",
    "review_system_template_str = \"\"\"Your job is to use patient\n",
    "reviews to answer questions about their experience at a\n",
    "hospital. Use the following context to answer questions.\n",
    "Be as detailed as possible, but don't make up any information\n",
    "that's not from the context. If you don't know an answer, say\n",
    "you don't know.\n",
    "\n",
    "Patient reviews:\n",
    "\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "review_system_prompt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate.from_template(review_system_template_str)\n",
    ")\n",
    "print (review_system_prompt)\n",
    "\n",
    "review_human_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate.from_template(\"{question}\")\n",
    ")\n",
    "print(review_human_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Your job is to use patient\\nreviews to answer questions about their experience at a\\nhospital. Use the following context to answer questions.\\nBe as detailed as possible, but don't make up any information\\nthat's not from the context. If you don't know an answer, say\\nyou don't know.\\n\\nPatient reviews:\\n\\n{context}\\n\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "messages = [review_system_prompt, review_human_prompt]\n",
    "review_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "review_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Yes, one patient mentioned having a great stay, which indicates a positive experience.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 86, 'total_tokens': 102, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9bddfca6e2', 'id': 'chatcmpl-BbuAlMQZz6uyDheTOC67BSXzwncV7', 'finish_reason': 'stop', 'logprobs': None} id='run-5f44eb0a-6bc7-447a-b186-61b20d7748b4-0' usage_metadata={'input_tokens': 86, 'output_tokens': 16, 'total_tokens': 102, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "context = \"I had a great stay!\"\n",
    "question = \"Did anyone have a positive experience?\"\n",
    "\n",
    "filled_prompt = review_prompt_template.format_messages(\n",
    "    context=context, question=question\n",
    ")\n",
    "\n",
    "print(chat_model.invoke(filled_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains\n",
    "Chains are a core concept in LangChain, representing an end-to-end wrapper around multiple individual components executed in a defined order. They allow developers to go beyond a single API call to an LLM and instead chain together multiple calls in a logical sequence. This enables breaking down complex tasks into smaller steps, maintaining context between calls by feeding the output of one step as input to the next, and adding intermediate processing logic. \n",
    "\n",
    "LangChain offers various types of chains:\n",
    "\n",
    "- LLM Chain: The simplest form, consisting of a PromptTemplate, a language model (LLM or ChatModel), and an optional output parser. We can see here the steps and how they feed on the output of the previous one: \n",
    "    - takes the input parameters\n",
    "    - uses the PromptTemplate to format them into a prompt\n",
    "    - passes the prompt to the model\n",
    "    - optionally uses the OutputParser to refine the result.\n",
    "- Sequential Chain: Combines various individual chains where the output of one chain serves as the input for the next in a continuous sequence. There are two types: Simple Sequential Chains, which handle a single input and output, and a more general form that allows for multiple inputs/outputs.\n",
    "- Router Chain: Used for complicated tasks when there are multiple subchains specialized for different types of input. It consists of a Router Chain that selects the next chain, Destination Chains that it can route to, and a Default Chain used when the router cannot decide. This adds intelligent decision-making by directing inputs to the most suitable processing paths.\n",
    "- Transform Chain: Applies a data transformation between chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Yes, the patient had a positive experience, as indicated by their statement, \"I had a great stay!\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 87, 'total_tokens': 109, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_76544d79cb', 'id': 'chatcmpl-BbuApcPrCxYIG9IIFpYuoGr73gvqj', 'finish_reason': 'stop', 'logprobs': None} id='run-79694fe5-992f-4880-b778-86c140faded7-0' usage_metadata={'input_tokens': 87, 'output_tokens': 22, 'total_tokens': 109, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content=\"I'm sorry, the context provided does not include information about the reason for the patient's stay or what they were interned for.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 86, 'total_tokens': 111, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-BbuAqpQ4B43CgsnrRjYQh3ulFfMAo', 'finish_reason': 'stop', 'logprobs': None} id='run-e8d9b843-775a-4890-9f91-7f545dc3fcdb-0' usage_metadata={'input_tokens': 86, 'output_tokens': 25, 'total_tokens': 111, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content=\"I'm sorry, but the context provided does not contain information on how to fix a car. It only mentions a patient review about needing a car fixed due to a punctured tire. For instructions on fixing a car, especially a punctured tire, you might want to consult a car repair manual or seek advice from a professional mechanic.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 91, 'total_tokens': 157, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-BbuArpY6TddborNYWw8E3zi92x3t2', 'finish_reason': 'stop', 'logprobs': None} id='run-501c1672-8b6c-4553-b806-f6c626d56f06-0' usage_metadata={'input_tokens': 91, 'output_tokens': 66, 'total_tokens': 157, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# define the chain\n",
    "review_chain = review_prompt_template | chat_model\n",
    "\n",
    "context = \"I had a great stay!\"\n",
    "question = \"Did the patient have a positive experience?\"\n",
    "# invoke it passing the parameters of the template for the first step\n",
    "print(review_chain.invoke({\"context\": context, \"question\": question}))\n",
    "\n",
    "question = \"what was he interned for?\"\n",
    "print(review_chain.invoke({\"context\": context, \"question\": question}))\n",
    "\n",
    "context = \"I need my car fixed. it has a punctured tire\"\n",
    "question = \"how to fix his car\"\n",
    "print(review_chain.invoke({\"context\": context, \"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting a chain\n",
    "For complex chains it will be good to have on your toolbelt the langchain_core.globals > set_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"context\": \"I need my car fixed. it has a punctured tire\",\n",
      "  \"question\": \"how to fix his car\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"context\": \"I need my car fixed. it has a punctured tire\",\n",
      "  \"question\": \"how to fix his car\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Your job is to use patient\\nreviews to answer questions about their experience at a\\nhospital. Use the following context to answer questions.\\nBe as detailed as possible, but don't make up any information\\nthat's not from the context. If you don't know an answer, say\\nyou don't know.\\n\\nPatient reviews:\\n\\nI need my car fixed. it has a punctured tire\\n\\nHuman: how to fix his car\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [1.86s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I'm sorry, but the context provided does not include information on how to fix a car. It only mentions a patient review about needing a car fixed due to a punctured tire. For car repair instructions, you might want to consult a car repair manual or a professional mechanic.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I'm sorry, but the context provided does not include information on how to fix a car. It only mentions a patient review about needing a car fixed due to a punctured tire. For car repair instructions, you might want to consult a car repair manual or a professional mechanic.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 55,\n",
      "                \"prompt_tokens\": 91,\n",
      "                \"total_tokens\": 146,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "              \"system_fingerprint\": \"fp_76544d79cb\",\n",
      "              \"id\": \"chatcmpl-BbuAtwsDFScnAJmJB1ntrh77ENvaA\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-b124e41b-8dff-40e3-8c5b-5d97bc06d60a-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 91,\n",
      "              \"output_tokens\": 55,\n",
      "              \"total_tokens\": 146,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 55,\n",
      "      \"prompt_tokens\": 91,\n",
      "      \"total_tokens\": 146,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "    \"system_fingerprint\": \"fp_76544d79cb\",\n",
      "    \"id\": \"chatcmpl-BbuAtwsDFScnAJmJB1ntrh77ENvaA\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [1.86s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "content=\"I'm sorry, but the context provided does not include information on how to fix a car. It only mentions a patient review about needing a car fixed due to a punctured tire. For car repair instructions, you might want to consult a car repair manual or a professional mechanic.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 91, 'total_tokens': 146, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_76544d79cb', 'id': 'chatcmpl-BbuAtwsDFScnAJmJB1ntrh77ENvaA', 'finish_reason': 'stop', 'logprobs': None} id='run-b124e41b-8dff-40e3-8c5b-5d97bc06d60a-0' usage_metadata={'input_tokens': 91, 'output_tokens': 55, 'total_tokens': 146, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.globals import set_debug\n",
    "set_debug(True)\n",
    "\n",
    "print(review_chain.invoke({\"context\": context, \"question\": question}))\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output parser\n",
    "So far we have in our chain a ChatPromptTemplate (review_prompt_template) and an LLM, in our case a ChatOpenAI (chat_model): We can add to the end a third block, an output parser to ease the reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient described their stay as great.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# redefine the chain\n",
    "review_chain = review_prompt_template | chat_model | output_parser\n",
    "\n",
    "#fill the parameters and invoke\n",
    "context = \"I had a great stay! I had terrible stomach pain. I ache for 3 days and a stone was found and removed from my liver\"\n",
    "question = \"How was his stay?\"\n",
    "response = review_chain.invoke({\"context\": context, \"question\": question})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of reviews I have access to is 24.\n",
      "\n",
      "Pure Positive Reviews: These reviews are overwhelmingly positive without mentioning any significant drawbacks.\n",
      "1. Review by Christy Johnson\n",
      "2. Review by Anna Frazier\n",
      "3. Review by Abigail Mitchell\n",
      "4. Review by Cody Ibarra\n",
      "5. Review by Scott Terry\n",
      "\n",
      "Pure Negative Reviews: These reviews are overwhelmingly negative without mentioning any positive aspects.\n",
      "1. Review by Rachel Carter\n",
      "\n",
      "Mixed Reviews: These reviews contain both positive and negative aspects.\n",
      "1. Review by Kimberly Rivas\n",
      "2. Review by Catherine Yang\n",
      "3. Review by Jennifer Russell\n",
      "4. Review by Henry Hays\n",
      "5. Review by Kim Franklin (two reviews)\n",
      "6. Review by Michael Smith (two reviews)\n",
      "7. Review by Chelsea Mitchell\n",
      "8. Review by Carol Byrd\n",
      "9. Review by Daniel Williams\n",
      "10. Review by Kim Powers\n",
      "11. Review by Sharon Brown\n",
      "12. Review by John Bartlett\n",
      "13. Review by Rebecca Wilkerson\n",
      "14. Review by Michele Jones\n",
      "15. Review by Tiffany Long\n",
      "16. Review by Stacy Villa\n",
      "\n",
      "In summary:\n",
      "- Total Reviews: 24\n",
      "- Pure Positive Reviews: 5\n",
      "- Pure Negative Reviews: 1\n",
      "- Mixed Reviews: 18\n"
     ]
    }
   ],
   "source": [
    "from reviews import reviews\n",
    "response = review_chain.invoke({\"context\": reviews, \"question\": \"tell me the total number of reviews, the number of pure positive ones, the number of puere negative ones and the number of mixed ones that you have access to ?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The doctor most mentioned in the reviews is Kyle Vasquez, with two reviews. The comments about him are as follows:\n",
      "\n",
      "1. Review by Kim Franklin (Visit ID: 3332): \"The hospital provided exceptional care, and the nursing staff was incredibly supportive. However, the administrative processes were a bit convoluted, causing some confusion.\" This review is generally positive about the care provided but notes an issue with the administrative processes.\n",
      "\n",
      "2. Another review by Kim Franklin (Visit ID: 3332): \"The hospital staff was efficient, and the facilities were clean. However, the lack of communication about my treatment plan was frustrating.\" This review is mixed, highlighting the efficiency and cleanliness but expressing frustration over communication issues regarding the treatment plan.\n"
     ]
    }
   ],
   "source": [
    "response = review_chain.invoke({\"context\": reviews, \"question\": \"who is the doctor most mentioned and what are the comments about him, good or bad?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
