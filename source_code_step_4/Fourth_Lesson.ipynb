{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4\n",
    "## New .env variables\n",
    "you need to add three new variables to you r .env file, if you didnt have done so yet\n",
    "\n",
    "```env\n",
    "HOSPITAL_AGENT_MODEL=gpt-3.5-turbo-1106\n",
    "HOSPITAL_CYPHER_MODEL=gpt-3.5-turbo-1106\n",
    "HOSPITAL_QA_MODEL=gpt-3.5-turbo-0125\n",
    "```\n",
    "Your .env file now includes variables that specify which LLM you’ll use for different components of your chatbot. You’ve specified these models as environment variables so that you can easily switch between different OpenAI models without changing any code. Keep in mind, however, that each LLM might benefit from a unique prompting strategy, so you might need to modify your prompts if you plan on using a different suite of LLMs.\n",
    "\n",
    "## RAG search.\n",
    "We will demonstrate how to implement a RAG pipeline where user questions about hospital reviews are answered using both retrieval (vector search) and generation (LLM).\n",
    "To do that we will create a Document Chain +LLM Chain that implement RAG to manage tue queries about the reviews and we will use Neo4j as the vector database.\n",
    "\n",
    "### Vector Search Indexes\n",
    "Vector databases are specifically designed to store and query high-dimensional vectors, which are numerical representations of unstructured data like text, images, or audio. Vector search indexes enable efficient similarity searches (semantic searches) based on that semantically similar objects are represented by vectors that are close to each other in the vector space.\n",
    "\n",
    "Neo4j, a graph database, has added vector search capabilities as an add-on. Neo4j's [Vector search indexes](https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/) allows developers to create vector indexes. These indexes work with node or relationship properties that contain vector embeddings\n",
    "\n",
    "### Creating the embeddings\n",
    "In LangChain, you can use Neo4jVector to create review embeddings and returns a \"vector store\" to query it. Here’s the code to create the indexes.\n",
    "\n",
    "The method takes several parameters:\n",
    "- embedding: an instance of OpenAIEmbeddings, which will be used to generate embeddings for the text data. The specific embedding model used will depend on the requirements of your project, such as the type of text data, the desired level of accuracy, and the computational resources available. We use it as we have openai api key but there are others in the langchain library.\n",
    "    - HuggingFaceEmbeddings: uses Hugging Face's Transformers library to generate embeddings\n",
    "    - SentenceTransformersEmbeddings: uses the Sentence Transformers library to generate embeddings\n",
    "    - CustomEmbeddings: a custom implementation of the Embeddings interface, allowing you to use your own embedding model\n",
    "    - Word2VecEmbeddings\n",
    "    - GloVeEmbeddings\n",
    "    - FastTextEmbeddings\n",
    "    - BERTEmbeddings\n",
    "- url, username, password: these are used to connect to the Neo4j database, and are retrieved from environment variables (NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD).\n",
    "- index_name: the name of the vector index to create (\"reviews\").\n",
    "- node_label: the label of the nodes in the graph database that will be indexed (\"Review\").\n",
    "- text_node_properties: a list of properties on the nodes that contain text data to be indexed ([\"physician_name\", \"patient_name\", \"text\", \"hospital_name\"]).\n",
    "- embedding_node_property: the property on the nodes where the generated embeddings will be stored (\"embedding\").\n",
    "\n",
    "---\n",
    "> ⚠️ **Note:** `from_existing_graph` is primarily used when your text data is already in the Neo4j graph, but the embeddings and the vector index might not yet exist or need to be generated by the LangChain process. It seems to automate querying Neo4j to get the relevant text data, calculate embeddings for that text, storing those newly generated embeddings back into the specified node property in Neo4j and creating the necessary vector index if it doesn't exist. Therefore, if the embeddings and the corresponding vector index already fully exist and are populated in your Neo4j database, the method you would typically use is `Neo4jVector.from_existing_index`, not `from_existing_graph`\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated procedure. ('db.create.setVectorProperty' has been replaced by 'db.create.setNodeVectorProperty')} {position: line: 1, column: 68, offset: 67} for query: \"UNWIND $data AS row MATCH (n:`Review`) WHERE elementId(n) = row.id CALL db.create.setVectorProperty(n, 'embedding', row.embedding) YIELD node RETURN count(*)\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "neo4j_vector_index = Neo4jVector.from_existing_graph(\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    url=os.getenv(\"NEO4J_URI\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    "    index_name=\"reviews\",\n",
    "    node_label=\"Review\",\n",
    "    text_node_properties=[\n",
    "        \"physician_name\",\n",
    "        \"patient_name\",\n",
    "        \"text\",\n",
    "        \"hospital_name\",\n",
    "    ],\n",
    "    embedding_node_property=\"embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.neo4j_vector.Neo4jVector at 0x7fcbf4cc10d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neo4j_vector_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you filled the database in the previous lesson, after running this you will see that the embeddings had been created as an index of the review text \n",
    "\n",
    "![embeddings.png](embeddings.png)\n",
    "\n",
    "This allows you to answer questions like \"Which hospitals have had positive reviews?\" It also allows the LLM to tell you which patient and physician wrote reviews matching your question.\n",
    "\n",
    "### The retriever\n",
    "To use the Retrieval Augmented Generation we need to create a retriever based in this vector store we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "HOSPITAL_QA_MODEL = os.getenv(\"HOSPITAL_QA_MODEL\") # gpt-3.5-turbo-0125\n",
    "\n",
    "# chat prompt template with system and human messages\n",
    "review_template = \"\"\"Your job is to use patient\n",
    "reviews to answer questions about their experience at a hospital. Use\n",
    "the following context to answer questions. Be as detailed as possible, but\n",
    "don't make up any information that's not from the context. If you don't know\n",
    "an answer, say you don't know.\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "review_system_prompt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(input_variables=[\"context\"], template=review_template)\n",
    ")\n",
    "\n",
    "review_human_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(input_variables=[\"question\"], template=\"{question}\")\n",
    ")\n",
    "messages = [review_system_prompt, review_human_prompt]\n",
    "\n",
    "review_prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], messages=messages\n",
    ")\n",
    "\n",
    "# the document chain, based in our vector index\n",
    "reviews_vector_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=HOSPITAL_QA_MODEL, temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=neo4j_vector_index.as_retriever(k=12),\n",
    ")\n",
    "\n",
    "# the complete review chain\n",
    "reviews_vector_chain.combine_documents_chain.llm_chain.prompt = review_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part is setting up a RetrievalQA chain, what is called a document chain.\n",
    "\n",
    "**The retriever/document chain**\n",
    "\n",
    "* `reviews_vector_chain` is the instance of `RetrievalQA`\n",
    "* The `from_chain_type` defines the chain type (pass all the docs found entirely, pass only relevant chunks of it, ...). In this case, the chain type is `\"stuff\"` the standard one, it passes all retrieved data as context. Different chain types are \"stuff\", \"map refine\", and \"rerank\", with the later are alternative complex ones to avoid overflowing the context window \n",
    "* The `llm` parameter specifies the language model that will be used specifically for the question-answering system and its temperature.\n",
    "* The `retriever` parameter specifies the vector store (retriever) that will be used to fish the relevant documents from the database. In this case, it's an instance of `neo4j_vector_index.as_retriever(k=12)`, which is a retriever that uses the Neo4j vector index to gather documents. The `k` parameter specifies the number of documents to retrieve (12).\n",
    "\n",
    "**Combining the documents chain and LLM chain**\n",
    "\n",
    "* The `combine_documents_chain` method is used to combine the documents chain (which retrieves relevant documents from the database) with the LLM chain (which uses the language model to generate answers).\n",
    "* The `llm_chain.prompt` parameter specifies the prompt that will be used for the LLM chain. In this case, it's set to the `review_prompt` instance that was defined earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Patients have generally praised the efficiency of the hospital staff and the cleanliness of the facilities. For example, Kevin Cox mentioned that the hospital staff at Wallace-Hamilton was efficient, and the facilities were clean. Similarly, Kim Franklin also noted that the hospital staff at Wallace-Hamilton was efficient, and the facilities were clean. These reviews highlight a positive aspect of the hospital's operations in terms of efficiency and cleanliness.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"What have patients said about hospital efficiency?\n",
    "        Mention details from specific reviews.\"\"\"\n",
    "\n",
    "response = reviews_vector_chain.invoke(query)\n",
    "response.get(\"result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB query search.\n",
    "In this part we will create the Cypher generation chain that you’ll use to answer queries about structured hospital system data.\n",
    "\n",
    "Neo4j Cypher chain will accept a user’s natural language query, convert the natural language query to a Cypher query, run the Cypher query in Neo4j, and use the Cypher query results to respond to the user’s query. You’ll leverage LangChain’s ```GraphCypherQAChain``` for this. \n",
    "\n",
    "If we were to use a normal SQL database we would use langchain's own ```SQLDatabase``` instead ```Neo4jGraph``` & ```SQLDatabaseChain``` instead ```GraphCypherQAChain```\n",
    "\n",
    "### DB Connection neo4j-style\n",
    "To get started we need first to have a LLM capable database connection with ```Neo4jGraph```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=os.getenv(\"NEO4J_URI\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    ")\n",
    "\n",
    "graph.refresh_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neo4jGraph object is a LangChain wrapper that allows LLMs to execute queries on your Neo4j instance. You instantiate graph using your Neo4j credentials, and you call graph.refresh_schema() to sync any recent changes to your instance.\n",
    "\n",
    "### cypher generation template. \n",
    "We want the LLM to generate a Cypher (neo4j query language) query based in a human prompt.\n",
    "\n",
    "This needs heavy prompting engineering and serious trial and error:\n",
    "- we need the answer to provide the query and only the query\n",
    "- It should only query, not insert, update or delete\n",
    "- we rather it use alias to make the query readable for debug purposes\n",
    "- we need to inform beforehand of the equivalent in the DB of certain status, businmess particulars that are not self evident from the schema i.e: \n",
    "    - visit is concluded if has a discharge date\n",
    "    - payment was rejected/pending if it does not have the service_date in the COVERED BY relationship\n",
    "    - Use abbreviations when filtering on hospital states (e.g. \"Texas\" is \"TX\", \"Colorado\" is \"CO\", \"North Carolina\" is \"NC\",...)\n",
    "    - list some categorical values of a few node properties for it to know beforehand\n",
    "    - etc.\n",
    "    \n",
    "All of the detail you provide in your prompt template improves the LLM’s chance of generating a correct Cypher query for a given question. If you’re curious about how necessary all this detail is, try creating your own prompt template with as few details as possible. Then run questions through your Cypher chain and see whether it correctly generates Cypher queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "cypher_generation_template = \"\"\"\n",
    "Task:\n",
    "Generate Cypher query for a Neo4j graph database.\n",
    "\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "Note:\n",
    "Do not include any explanations or apologies in your responses.\n",
    "Do not respond to any questions that might ask anything other than\n",
    "for you to construct a Cypher statement. Do not include any text except\n",
    "the generated Cypher statement. Make sure the direction of the relationship is\n",
    "correct in your queries. Make sure you alias both entities and relationships\n",
    "properly. Do not run any queries that would add to or delete from\n",
    "the database. Make sure to alias all statements that follow as with\n",
    "statement (e.g. WITH v as visit, c.billing_amount as billing_amount)\n",
    "If you need to divide numbers, make sure to\n",
    "filter the denominator to be non zero.\n",
    "\n",
    "Examples:\n",
    "# Who is the oldest patient and how old are they?\n",
    "MATCH (p:Patient)\n",
    "RETURN p.name AS oldest_patient,\n",
    "       duration.between(date(p.dob), date()).years AS age\n",
    "ORDER BY age DESC\n",
    "LIMIT 1\n",
    "\n",
    "# Which physician has billed the least to Cigna\n",
    "MATCH (p:Payer)<-[c:COVERED_BY]-(v:Visit)-[t:TREATS]-(phy:Physician)\n",
    "WHERE p.name = 'Cigna'\n",
    "RETURN phy.name AS physician_name, SUM(c.billing_amount) AS total_billed\n",
    "ORDER BY total_billed\n",
    "LIMIT 1\n",
    "\n",
    "# Which state had the largest percent increase in Cigna visits\n",
    "# from 2022 to 2023?\n",
    "MATCH (h:Hospital)<-[:AT]-(v:Visit)-[:COVERED_BY]->(p:Payer)\n",
    "WHERE p.name = 'Cigna' AND v.admission_date >= '2022-01-01' AND\n",
    "v.admission_date < '2024-01-01'\n",
    "WITH h.state_name AS state, COUNT(v) AS visit_count,\n",
    "     SUM(CASE WHEN v.admission_date >= '2022-01-01' AND\n",
    "     v.admission_date < '2023-01-01' THEN 1 ELSE 0 END) AS count_2022,\n",
    "     SUM(CASE WHEN v.admission_date >= '2023-01-01' AND\n",
    "     v.admission_date < '2024-01-01' THEN 1 ELSE 0 END) AS count_2023\n",
    "WITH state, visit_count, count_2022, count_2023,\n",
    "     (toFloat(count_2023) - toFloat(count_2022)) / toFloat(count_2022) * 100\n",
    "     AS percent_increase\n",
    "RETURN state, percent_increase\n",
    "ORDER BY percent_increase DESC\n",
    "LIMIT 1\n",
    "\n",
    "# How many non-emergency patients in North Carolina have written reviews?\n",
    "MATCH (r:Review)<-[:WRITES]-(v:Visit)-[:AT]->(h:Hospital)\n",
    "WHERE h.state_name = 'NC' and v.admission_type <> 'Emergency'\n",
    "RETURN count(*)\n",
    "\n",
    "String category values:\n",
    "Test results are one of: 'Inconclusive', 'Normal', 'Abnormal'\n",
    "Visit statuses are one of: 'OPEN', 'DISCHARGED'\n",
    "Admission Types are one of: 'Elective', 'Emergency', 'Urgent'\n",
    "Payer names are one of: 'Cigna', 'Blue Cross', 'UnitedHealthcare', 'Medicare',\n",
    "'Aetna'\n",
    "\n",
    "A visit is considered open if its status is 'OPEN' and the discharge date is\n",
    "missing.\n",
    "Use abbreviations when\n",
    "filtering on hospital states (e.g. \"Texas\" is \"TX\",\n",
    "\"Colorado\" is \"CO\", \"North Carolina\" is \"NC\",\n",
    "\"Florida\" is \"FL\", \"Georgia\" is \"GA\", etc.)\n",
    "\n",
    "Make sure to use IS NULL or IS NOT NULL when analyzing missing properties.\n",
    "Never return embedding properties in your queries. You must never include the\n",
    "statement \"GROUP BY\" in your query. Make sure to alias all statements that\n",
    "follow as with statement (e.g. WITH v as visit, c.billing_amount as\n",
    "billing_amount)\n",
    "If you need to divide numbers, make sure to filter the denominator to be non\n",
    "zero.\n",
    "\n",
    "The question is:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "cypher_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=cypher_generation_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "> ⚠️ **Note:** The above prompt template provides the LLM with four examples of valid Cypher queries for your graph. Giving the LLM a few examples and then asking it to perform a task is known as few-shot prompting, and it’s a simple yet powerful technique for improving generation accuracy. However, few-shot prompting might not be sufficient for Cypher query generation, especially if you have a complicated graph. One way to improve this is to create a vector database that embeds example user questions/queries and stores their corresponding Cypher queries as metadata. When a user asks a question, you inject Cypher queries from semantically similar questions into the prompt, providing the LLM with the most relevant examples needed to answer the current question.\n",
    "---\n",
    "\n",
    "### cypher results interpretation template\n",
    "Next, you define the prompt template for the question-answer component of your chain. This template tells the LLM to use the Cypher query results from launching the query obtained with the previous prompt to generate a nicely-formatted answer to the user’s query. Again heavy prompting required, but not quite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_generation_template = \"\"\"You are an assistant that takes the results\n",
    "from a Neo4j Cypher query and forms a human-readable response. The\n",
    "query results section contains the results of a Cypher query that was\n",
    "generated based on a user's natural language question. The provided\n",
    "information is authoritative, you must never doubt it or try to use\n",
    "your internal knowledge to correct it. Make the answer sound like a\n",
    "response to the question.\n",
    "\n",
    "Query Results:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "If the provided information is empty, say you don't know the answer.\n",
    "Empty information looks like this: []\n",
    "\n",
    "If the information is not empty, you must provide an answer using the\n",
    "results. If the question involves a time duration, assume the query\n",
    "results are in units of days unless otherwise specified.\n",
    "\n",
    "When names are provided in the query results, such as hospital names,\n",
    "beware  of any names that have commas or other punctuation in them.\n",
    "For instance, 'Jones, Brown and Murray' is a single hospital name,\n",
    "not multiple hospitals. Make sure you return any list of names in\n",
    "a way that isn't ambiguous and allows someone to tell what the full\n",
    "names are.\n",
    "\n",
    "Never say you don't have the right information if there is data in\n",
    "the query results. Always use the data in the query results.\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "qa_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], template=qa_generation_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GraphCypherQAChain\n",
    "This chain will integrate both parts: the creation of the query and the parse of the results retrieved by launching it.\n",
    "\n",
    "Here’s a breakdown of the parameters used in GraphCypherQAChain.from_llm():\n",
    "\n",
    "- cypher_llm: The LLM used to generate Cypher queries.\n",
    "- qa_llm: The LLM used to generate an answer given Cypher query results.\n",
    "- graph: The Neo4jGraph object that connects to your Neo4j instance. The database connection if you will.\n",
    "- verbose: Whether intermediate steps your chain performs should be printed.\n",
    "- qa_prompt: The prompt template for responding to questions/queries.\n",
    "- cypher_prompt: The prompt template for generating Cypher queries.\n",
    "- validate_cypher: If true, the Cypher query will be inspected for errors and corrected before running. Note that this doesn’t guarantee the Cypher query will be valid. Instead, it corrects simple syntax errors that are easily detectable using regular expressions.\n",
    "- top_k: The number of query results to include in qa_prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "HOSPITAL_QA_MODEL = os.getenv(\"HOSPITAL_QA_MODEL\")\n",
    "HOSPITAL_CYPHER_MODEL = os.getenv(\"HOSPITAL_CYPHER_MODEL\")\n",
    "\n",
    "hospital_cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm=ChatOpenAI(model=HOSPITAL_CYPHER_MODEL, temperature=0),\n",
    "    qa_llm=ChatOpenAI(model=HOSPITAL_QA_MODEL, temperature=0),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    qa_prompt=qa_generation_prompt,\n",
    "    cypher_prompt=cypher_generation_prompt,\n",
    "    validate_cypher=True,\n",
    "    top_k=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (v:Visit)-[:AT]->(h:Hospital)\n",
      "WHERE h.state_name = 'NC' AND v.admission_type = 'Emergency' AND v.status = 'DISCHARGED'\n",
      "WITH v, duration.between(date(v.admission_date), date(v.discharge_date)).days AS visit_duration\n",
      "RETURN AVG(visit_duration) AS average_visit_duration\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'average_visit_duration': 15.072972972972991}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The average visit duration for emergency visits in North Carolina is approximately 15.07 days.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"What is the average visit duration for emergency visits in North Carolina?\"\"\"\n",
    "response = hospital_cypher_chain.invoke(question)\n",
    "response.get(\"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (h:Hospital)<-[:AT]-(v:Visit)-[:COVERED_BY]->(p:Payer)\n",
      "WHERE p.name = 'Medicaid' AND v.admission_date >= '2022-01-01' AND\n",
      "v.admission_date < '2024-01-01'\n",
      "WITH h.state_name AS state, COUNT(v) AS visit_count,\n",
      "     SUM(CASE WHEN v.admission_date >= '2022-01-01' AND\n",
      "     v.admission_date < '2023-01-01' THEN 1 ELSE 0 END) AS count_2022,\n",
      "     SUM(CASE WHEN v.admission_date >= '2023-01-01' AND\n",
      "     v.admission_date < '2024-01-01' THEN 1 ELSE 0 END) AS count_2023\n",
      "WITH state, visit_count, count_2022, count_2023,\n",
      "     (toFloat(count_2023) - toFloat(count_2022)) / toFloat(count_2022) * 100\n",
      "     AS percent_increase\n",
      "RETURN state, percent_increase\n",
      "ORDER BY percent_increase DESC\n",
      "LIMIT 1\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'state': 'TX', 'percent_increase': 8.823529411764707}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The state with the largest percent increase in Medicaid visits from 2022 to 2023 is Texas (TX) with a percent increase of 8.82%.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"Which state had the largest percent increase in Medicaid visits from 2022 to 2023?\"\"\"\n",
    "response = hospital_cypher_chain.invoke(question)\n",
    "response.get(\"result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait Time Functions/Search\n",
    "This part will be to simulate the LLM interactions with other APIs, trully agent-like. It will answer questions about hospital wait times. You’ll write two functions for this: \n",
    "- one that simulates finding the current wait time at a hospital\n",
    "- and another that finds the hospital with the shortest wait time.\n",
    "\n",
    "### Hospital name retriever\n",
    "First we need to get the hospital names from the same Neo4j database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['burke, griffin and cooper',\n",
       " 'walton llc',\n",
       " 'garcia ltd',\n",
       " 'jones, brown and murray',\n",
       " 'boyd plc']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "def _get_current_hospitals() -> list[str]:\n",
    "    \"\"\"Fetch a list of current hospital names from a Neo4j database.\"\"\"\n",
    "    graph = Neo4jGraph(\n",
    "        url=os.getenv(\"NEO4J_URI\"),\n",
    "        username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "        password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    "    )\n",
    "\n",
    "    current_hospitals = graph.query(\n",
    "        \"\"\"\n",
    "        MATCH (h:Hospital)\n",
    "        RETURN h.name AS hospital_name\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    return [d[\"hospital_name\"].lower() for d in current_hospitals]\n",
    "\n",
    "_get_current_hospitals()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time functions\n",
    "(Simulated) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any\n",
    "def _get_current_wait_time_minutes(hospital: str) -> int:\n",
    "    \"\"\"\n",
    "    Get the current wait time at a hospital in minutes.\n",
    "    This will be a real API call in an application.\n",
    "    \"\"\"\n",
    "    current_hospitals = _get_current_hospitals()\n",
    "\n",
    "    if hospital.lower() not in current_hospitals:\n",
    "        return -1\n",
    "\n",
    "    return np.random.randint(low=0, high=600)\n",
    "\n",
    "\n",
    "def get_current_wait_times(hospital: str) -> str:\n",
    "    \"\"\"Get the current wait time at a hospital formatted as a string.\"\"\"\n",
    "    wait_time_in_minutes = _get_current_wait_time_minutes(hospital)\n",
    "\n",
    "    if wait_time_in_minutes == -1:\n",
    "        return f\"Hospital '{hospital}' does not exist.\"\n",
    "\n",
    "    hours, minutes = divmod(wait_time_in_minutes, 60)\n",
    "\n",
    "    if hours > 0:\n",
    "        return f\"{hours} hours {minutes} minutes\"\n",
    "    else:\n",
    "        return f\"{minutes} minutes\"\n",
    "    \n",
    "\n",
    "def get_most_available_hospital(_: Any) -> dict[str, float]:\n",
    "    \"\"\"Find the hospital with the shortest wait time.\"\"\"\n",
    "    current_hospitals = _get_current_hospitals()\n",
    "\n",
    "    current_wait_times = [\n",
    "        _get_current_wait_time_minutes(h) for h in current_hospitals\n",
    "    ]\n",
    "\n",
    "    best_time_idx = np.argmin(current_wait_times)\n",
    "    best_hospital = current_hospitals[best_time_idx]\n",
    "    best_wait_time = current_wait_times[best_time_idx]\n",
    "\n",
    "    return {best_hospital: best_wait_time}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 hours 34 minutes'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_wait_times(\"Wallace-Hamilton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bell, mcknight and willis': 34}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_available_hospital(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the agent\n",
    "We are going to create the agent. Depending on the query you give it, your agent needs to decide between your Cypher/DB Query chain, reviews/RAG chain, and wait times/API functions. Another tuesday for an AI Agent.\n",
    "\n",
    "Start by loading your agent’s dependencies, reading in the agent model name from an environment variable, and loading a prompt template from LangChain Hub: https://smith.langchain.com/hub/hwchase17/openai-functions-agent\n",
    "\n",
    "HOSPITAL_AGENT_MODEL is the LLM that will act as your agent’s brain, deciding which tools to call and what inputs to pass them.\n",
    "\n",
    "Instead of defining your own prompt for the agent, which you can certainly do, you load a predefined prompt from LangChain Hub. LangChain hub lets you upload, browse, pull, test, and manage prompts. In this case, the default prompt for OpenAI function agents works great.\n",
    "\n",
    "### The agent prompt\n",
    "The prompt is \n",
    "```python\n",
    "ChatPromptTemplate(\n",
    "    input_variables=['agent_scratchpad', 'input'], \n",
    "    input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, \n",
    "                                            langchain_core.messages.human.HumanMessage, \n",
    "                                            langchain_core.messages.chat.ChatMessage, \n",
    "                                            langchain_core.messages.system.SystemMessage, \n",
    "                                            langchain_core.messages.function.FunctionMessage, \n",
    "                                            langchain_core.messages.tool.ToolMessage]], \n",
    "                'agent_scratchpad': typing.List[... again all messages ...]\n",
    "    }, \n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                    input_variables=[], \n",
    "                    template='You are a helpful assistant'\n",
    "                )\n",
    "        ), \n",
    "        MessagesPlaceholder(\n",
    "            variable_name='chat_history', \n",
    "            optional=True\n",
    "        ), \n",
    "        HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                    input_variables=['input'], \n",
    "                    template='{input}'\n",
    "                )\n",
    "        ), \n",
    "        MessagesPlaceholder(\n",
    "            variable_name='agent_scratchpad'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "```\n",
    "**Overall Structure**\n",
    "\n",
    "This template creates a structured conversation flow with four main components arranged in order:\n",
    "\n",
    "**System instructions**\n",
    "\n",
    "- Chat history (optional)\n",
    "- Current user input\n",
    "- Agent scratchpad\n",
    "\n",
    "**Key Components Explained**\n",
    "\n",
    "1. agent_scratchpad\n",
    "This is the \"working memory\" of the agent during tool usage. It contains the sequence of:\n",
    "- Tool calls the agent makes\n",
    "- Tool responses from those calls\n",
    "- Agent reasoning between tool uses\n",
    "\n",
    "Example flow:\n",
    "```shell\n",
    "User: \"What's the weather in Paris?\"\n",
    "Agent: I'll check the weather for you.\n",
    "[Tool Call: weather_api(\"Paris\")]\n",
    "[Tool Response: \"22°C, sunny\"]\n",
    "Agent: The weather in Paris is 22°C and sunny.\n",
    "```\n",
    "The scratchpad captures this entire reasoning chain, allowing the agent to:\n",
    "\n",
    "Track what tools it has used\n",
    "- Remember tool results\n",
    "- Make multi-step reasoning decisions\n",
    "- Avoid repeating the same tool calls\n",
    "\n",
    "2. chat_history\n",
    "This maintains the conversation context across multiple turns. It stores the complete conversation history as a list of messages (Human, AI, System, etc.).\n",
    "Purpose:\n",
    "- Enables follow-up questions (\"What about tomorrow?\")\n",
    "- Maintains context (\"Paris\" from previous question)\n",
    "- Allows referencing earlier parts of conversation\n",
    "\n",
    "3. Input Variables & Types\n",
    "- input: The current user question/request\n",
    "- agent_scratchpad: The tool usage history for current turn\n",
    "\n",
    "4. Message Flow\n",
    "\n",
    "- SystemMessagePromptTemplate: Sets the agent's role (\"You are a helpful assistant\")\n",
    "- MessagesPlaceholder(chat_history): Injects previous conversation turns (marked optional=True)\n",
    "- HumanMessagePromptTemplate: The current user input\n",
    "- MessagesPlaceholder(agent_scratchpad): The agent's tool usage and reasoning for this turn\n",
    "\n",
    "**Why This Structure?**\n",
    "\n",
    "This template enables ReAct-style reasoning (Reasoning + Acting):\n",
    "\n",
    "- Agent receives input\n",
    "- Can use tools to gather information\n",
    "- Reasons about tool results\n",
    "- Provides final response\n",
    "- All while maintaining conversation context\n",
    "\n",
    "The scratchpad is essential because it allows the LLM to see its own \"thought process\" and tool interactions, enabling more sophisticated multi-step problem solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import (\n",
    "    create_openai_functions_agent,\n",
    "    Tool,\n",
    "    AgentExecutor,\n",
    ")\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "HOSPITAL_AGENT_MODEL = os.getenv(\"HOSPITAL_AGENT_MODEL\")\n",
    "\n",
    "hospital_agent_prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "hospital_agent_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Tools\n",
    "\n",
    "Your agent has four tools available to it: Experiences, Graph, Waits, and Availability. The Experiences and Graph tools call .invoke() from their respective chains, while Waits and Availability call the wait time functions you defined. Notice that many of the tool descriptions have few-shot prompts, telling the agent when it should use the tool and providing it with an example of what inputs to pass.\n",
    "\n",
    "As with chains, good prompt engineering is crucial for your agent’s success. You have to clearly describe each tool and how to use it so that your agent isn’t confused by a query and choose the wrong tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Experiences\",\n",
    "        func=reviews_vector_chain.invoke,\n",
    "        description=\"\"\"Useful when you need to answer questions\n",
    "        about patient experiences, feelings, or any other qualitative\n",
    "        question that could be answered about a patient using semantic\n",
    "        search. Not useful for answering objective questions that involve\n",
    "        counting, percentages, aggregations, or listing facts. Use the\n",
    "        entire prompt as input to the tool. For instance, if the prompt is\n",
    "        \"Are patients satisfied with their care?\", the input should be\n",
    "        \"Are patients satisfied with their care?\".\n",
    "        \"\"\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Graph\",\n",
    "        func=hospital_cypher_chain.invoke,\n",
    "        description=\"\"\"Useful for answering questions about patients,\n",
    "        physicians, hospitals, insurance payers, patient review\n",
    "        statistics, and hospital visit details. Use the entire prompt as\n",
    "        input to the tool. For instance, if the prompt is \"How many visits\n",
    "        have there been?\", the input should be \"How many visits have\n",
    "        there been?\".\n",
    "        \"\"\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Waits\",\n",
    "        func=get_current_wait_times,\n",
    "        description=\"\"\"Use when asked about current wait times\n",
    "        at a specific hospital. This tool can only get the current\n",
    "        wait time at a hospital and does not have any information about\n",
    "        aggregate or historical wait times. Do not pass the word \"hospital\"\n",
    "        as input, only the hospital name itself. For example, if the prompt\n",
    "        is \"What is the current wait time at Jordan Inc Hospital?\", the\n",
    "        input should be \"Jordan Inc\".\n",
    "        \"\"\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Availability\",\n",
    "        func=get_most_available_hospital,\n",
    "        description=\"\"\"\n",
    "        Use when you need to find out which hospital has the shortest\n",
    "        wait time. This tool does not have any information about aggregate\n",
    "        or historical wait times. This tool returns a dictionary with the\n",
    "        hospital name as the key and the wait time in minutes as the value.\n",
    "        \"\"\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the agent\n",
    "\"Always two, there are: the agent and the executor\"\n",
    "\n",
    "with create_openai_functions_agent(). This creates an agent that’s been designed by OpenAI to pass inputs to functions. It does this by returning JSON objects that store function inputs and their corresponding value.\n",
    "\n",
    "To create the agent run time, you pass your agent and tools into AgentExecutor. Setting return_intermediate_steps and verbose to true allows you to see the agent’s thought process and the tools it calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(\n",
    "    model=HOSPITAL_AGENT_MODEL,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "hospital_rag_agent = create_openai_functions_agent(\n",
    "    llm=chat_model,\n",
    "    prompt=hospital_agent_prompt,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "hospital_rag_agent_executor = AgentExecutor(\n",
    "    agent=hospital_rag_agent,\n",
    "    tools=tools,\n",
    "    return_intermediate_steps=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Waits` with `Wallace-Hamilton`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m7 hours 40 minutes\u001b[0m\u001b[32;1m\u001b[1;3mThe current wait time at Wallace-Hamilton is 7 hours and 40 minutes.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "What is the wait time at Wallace-Hamilton?=\n",
      "The current wait time at Wallace-Hamilton is 7 hours and 40 minutes.=\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(AgentActionMessageLog(tool='Waits', tool_input='Wallace-Hamilton', log='\\nInvoking: `Waits` with `Wallace-Hamilton`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"__arg1\":\"Wallace-Hamilton\"}', 'name': 'Waits'}})]),\n",
       "  '7 hours 40 minutes')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = hospital_rag_agent_executor.invoke(\n",
    "     {\"input\": \"What is the wait time at Wallace-Hamilton?\"}\n",
    " )\n",
    "\n",
    "print(f'{response[\"input\"]}=')\n",
    "print(f'{response[\"output\"]}=')\n",
    "response[\"intermediate_steps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Availability` with `shortest wait time`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'boyd plc': 37}\u001b[0m\u001b[32;1m\u001b[1;3mThe hospital with the shortest wait time is Boyd PLC, with a wait time of 37 minutes.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Which hospital has the shortest wait time?=\n",
      "The hospital with the shortest wait time is Boyd PLC, with a wait time of 37 minutes.=\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(AgentActionMessageLog(tool='Availability', tool_input='shortest wait time', log='\\nInvoking: `Availability` with `shortest wait time`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"__arg1\":\"shortest wait time\"}', 'name': 'Availability'}})]),\n",
       "  {'boyd plc': 37})]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = hospital_rag_agent_executor.invoke(\n",
    "     {\"input\": \"Which hospital has the shortest wait time?\"}\n",
    " )\n",
    "\n",
    "print(f'{response[\"input\"]}=')\n",
    "print(f'{response[\"output\"]}=')\n",
    "response[\"intermediate_steps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Experiences` with `quality of rest during their stay`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'quality of rest during their stay', 'result': \"Monique Foster at Pearson LLC mentioned that the hospital's cleanliness and hygiene standards were commendable. However, she found it challenging to get a good night's sleep due to constant interruptions for routine checks. Mrs. Kelly Berry DVM at Vaughn PLC also faced difficulty with rest due to higher-than-expected noise levels in the ward. Jesse Tucker at Wallace-Hamilton had a similar experience with constant interruptions during the night affecting their ability to rest. David Kim, also at Wallace-Hamilton, mentioned that while the hospital staff was accommodating, the disruptive noise levels in the hallway affected his ability to rest. Overall, it seems that the quality of rest during their stay was impacted by various factors such as noise levels and interruptions for routine checks.\"}\u001b[0m\u001b[32;1m\u001b[1;3mPatients have mentioned that the quality of rest during their stay was impacted by various factors such as noise levels and interruptions for routine checks. Monique Foster at Pearson LLC found it challenging to get a good night's sleep due to constant interruptions for routine checks. Mrs. Kelly Berry DVM at Vaughn PLC also faced difficulty with rest due to higher-than-expected noise levels in the ward. Similarly, Jesse Tucker at Wallace-Hamilton and David Kim, also at Wallace-Hamilton, mentioned that constant interruptions during the night affected their ability to rest.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "What have patients said about their quality of rest during their stay?=\n",
      "Patients have mentioned that the quality of rest during their stay was impacted by various factors such as noise levels and interruptions for routine checks. Monique Foster at Pearson LLC found it challenging to get a good night's sleep due to constant interruptions for routine checks. Mrs. Kelly Berry DVM at Vaughn PLC also faced difficulty with rest due to higher-than-expected noise levels in the ward. Similarly, Jesse Tucker at Wallace-Hamilton and David Kim, also at Wallace-Hamilton, mentioned that constant interruptions during the night affected their ability to rest.=\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(AgentActionMessageLog(tool='Experiences', tool_input='quality of rest during their stay', log='\\nInvoking: `Experiences` with `quality of rest during their stay`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"__arg1\":\"quality of rest during their stay\"}', 'name': 'Experiences'}})]),\n",
       "  {'query': 'quality of rest during their stay',\n",
       "   'result': \"Monique Foster at Pearson LLC mentioned that the hospital's cleanliness and hygiene standards were commendable. However, she found it challenging to get a good night's sleep due to constant interruptions for routine checks. Mrs. Kelly Berry DVM at Vaughn PLC also faced difficulty with rest due to higher-than-expected noise levels in the ward. Jesse Tucker at Wallace-Hamilton had a similar experience with constant interruptions during the night affecting their ability to rest. David Kim, also at Wallace-Hamilton, mentioned that while the hospital staff was accommodating, the disruptive noise levels in the hallway affected his ability to rest. Overall, it seems that the quality of rest during their stay was impacted by various factors such as noise levels and interruptions for routine checks.\"})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = hospital_rag_agent_executor.invoke(\n",
    "     {\"input\": \"What have patients said about their quality of rest during their stay?\"}\n",
    " )\n",
    "\n",
    "print(f'{response[\"input\"]}=')\n",
    "print(f'{response[\"output\"]}=')\n",
    "response[\"intermediate_steps\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
