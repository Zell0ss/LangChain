{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4\n",
    "## New .env variables\n",
    "you need to add three new variables to you r .env file, if you didnt have done so yet\n",
    "\n",
    "```env\n",
    "HOSPITAL_AGENT_MODEL=gpt-3.5-turbo-1106\n",
    "HOSPITAL_CYPHER_MODEL=gpt-3.5-turbo-1106\n",
    "HOSPITAL_QA_MODEL=gpt-3.5-turbo-0125\n",
    "```\n",
    "Your .env file now includes variables that specify which LLM you’ll use for different components of your chatbot. You’ve specified these models as environment variables so that you can easily switch between different OpenAI models without changing any code. Keep in mind, however, that each LLM might benefit from a unique prompting strategy, so you might need to modify your prompts if you plan on using a different suite of LLMs.\n",
    "\n",
    "## RAG search.\n",
    "In this part we will create a Document Chain +LLM Chain that implement RAG to manage que queries about the reviews, We will use Neo4j as the vector database.\n",
    "### Vector Search Indexes\n",
    "Vector databases are specifically designed to store and query high-dimensional vectors, which are numerical representations of unstructured data like text, images, or audio. Vector search indexes enable efficient similarity searches (semantic searches) based on that semantically similar objects are represented by vectors that are close to each other in the vector space.\n",
    "\n",
    "Neo4j, a graph database, has added vector search capabilities as an add-on. Neo4j's [Vector search indexes](https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/) allows developers to create vector indexes. These indexes work with node or relationship properties that contain vector embeddings\n",
    "\n",
    "### Creating the embeddings\n",
    "In LangChain, you can use Neo4jVector to create review embeddings and returns a \"vector store\" to query it. Here’s the code to create the indexes.\n",
    "\n",
    "The method takes several parameters:\n",
    "- embedding: an instance of OpenAIEmbeddings, which will be used to generate embeddings for the text data. The specific embedding model used will depend on the requirements of your project, such as the type of text data, the desired level of accuracy, and the computational resources available. We use it as we have openai api key but there are others in the langchain library.\n",
    "    - HuggingFaceEmbeddings: uses Hugging Face's Transformers library to generate embeddings\n",
    "    - SentenceTransformersEmbeddings: uses the Sentence Transformers library to generate embeddings\n",
    "    - CustomEmbeddings: a custom implementation of the Embeddings interface, allowing you to use your own embedding model\n",
    "    - Word2VecEmbeddings\n",
    "    - GloVeEmbeddings\n",
    "    - FastTextEmbeddings\n",
    "    - BERTEmbeddings\n",
    "- url, username, password: these are used to connect to the Neo4j database, and are retrieved from environment variables (NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD).\n",
    "- index_name: the name of the vector index to create (\"reviews\").\n",
    "- node_label: the label of the nodes in the graph database that will be indexed (\"Review\").\n",
    "- text_node_properties: a list of properties on the nodes that contain text data to be indexed ([\"physician_name\", \"patient_name\", \"text\", \"hospital_name\"]).\n",
    "- embedding_node_property: the property on the nodes where the generated embeddings will be stored (\"embedding\").\n",
    "\n",
    "---\n",
    "> ⚠️ **Note:** `from_existing_graph` is primarily used when your text data is already in the Neo4j graph, but the embeddings and the vector index might not yet exist or need to be generated by the LangChain process. It seems to automate querying Neo4j to get the relevant text data, calculate embeddings for that text, storing those newly generated embeddings back into the specified node property in Neo4j and creating the necessary vector index if it doesn't exist. Therefore, if the embeddings and the corresponding vector index already fully exist and are populated in your Neo4j database, the method you would typically use is `Neo4jVector.from_existing_index`, not `from_existing_graph`\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "neo4j_vector_index = Neo4jVector.from_existing_graph(\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    url=os.getenv(\"NEO4J_URI\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    "    index_name=\"reviews\",\n",
    "    node_label=\"Review\",\n",
    "    text_node_properties=[\n",
    "        \"physician_name\",\n",
    "        \"patient_name\",\n",
    "        \"text\",\n",
    "        \"hospital_name\",\n",
    "    ],\n",
    "    embedding_node_property=\"embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.neo4j_vector.Neo4jVector at 0x7fcbf4cc10d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neo4j_vector_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you filled the database in the previous lesson, after running this you will see that the embeddings had been created as an index of the review text \n",
    "\n",
    "![embeddings.png](embeddings.png)\n",
    "\n",
    "This allows you to answer questions like \"Which hospitals have had positive reviews?\" It also allows the LLM to tell you which patient and physician wrote reviews matching your question.\n",
    "\n",
    "### The retriever\n",
    "To use the Retrieval Augmented Generation we need to create a retriever based in this vector store we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "HOSPITAL_QA_MODEL = os.getenv(\"HOSPITAL_QA_MODEL\") # gpt-3.5-turbo-0125\n",
    "\n",
    "# chat prompt template with system and human messages\n",
    "review_template = \"\"\"Your job is to use patient\n",
    "reviews to answer questions about their experience at a hospital. Use\n",
    "the following context to answer questions. Be as detailed as possible, but\n",
    "don't make up any information that's not from the context. If you don't know\n",
    "an answer, say you don't know.\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "review_system_prompt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(input_variables=[\"context\"], template=review_template)\n",
    ")\n",
    "\n",
    "review_human_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(input_variables=[\"question\"], template=\"{question}\")\n",
    ")\n",
    "messages = [review_system_prompt, review_human_prompt]\n",
    "\n",
    "review_prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], messages=messages\n",
    ")\n",
    "\n",
    "# the document chain, based in our vector index\n",
    "reviews_vector_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=HOSPITAL_QA_MODEL, temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=neo4j_vector_index.as_retriever(k=12),\n",
    ")\n",
    "\n",
    "# the complete review chain\n",
    "reviews_vector_chain.combine_documents_chain.llm_chain.prompt = review_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part is setting up a RetrievalQA chain, what is called a document chain.\n",
    "\n",
    "**The retriever/document chain**\n",
    "\n",
    "* `reviews_vector_chain` is the instance of `RetrievalQA`\n",
    "* The `from_chain_type` defines the chain type (pass all the docs found entirely, pass only relevant chunks of it, ...). In this case, the chain type is `\"stuff\"` the standard one, it passes all retrieved data as context. Different chain types are \"stuff\", \"map refine\", and \"rerank\", with the later are alternative complex ones to avoid overflowing the context window \n",
    "* The `llm` parameter specifies the language model that will be used specifically for the question-answering system and its temperature.\n",
    "* The `retriever` parameter specifies the vector store (retriever) that will be used to fish the relevant documents from the database. In this case, it's an instance of `neo4j_vector_index.as_retriever(k=12)`, which is a retriever that uses the Neo4j vector index to gather documents. The `k` parameter specifies the number of documents to retrieve (12).\n",
    "\n",
    "**Combining the documents chain and LLM chain**\n",
    "\n",
    "* The `combine_documents_chain` method is used to combine the documents chain (which retrieves relevant documents from the database) with the LLM chain (which uses the language model to generate answers).\n",
    "* The `llm_chain.prompt` parameter specifies the prompt that will be used for the LLM chain. In this case, it's set to the `review_prompt` instance that was defined earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients have generally praised the efficiency of the hospital staff and the cleanliness of the facilities. For example, Kevin Cox mentioned that the hospital staff at Wallace-Hamilton was efficient, and Jesse Marquez also noted that the hospital provided excellent medical care at the same facility. Additionally, Kim Franklin commented on the efficiency of the hospital staff and the cleanliness of the facilities at Wallace-Hamilton.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"What have patients said about hospital efficiency?\n",
    "        Mention details from specific reviews.\"\"\"\n",
    "\n",
    "response = reviews_vector_chain.invoke(query)\n",
    "response.get(\"result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB query search.\n",
    "In this part we will create the Cypher generation chain that you’ll use to answer queries about structured hospital system data.\n",
    "\n",
    "Neo4j Cypher chain will accept a user’s natural language query, convert the natural language query to a Cypher query, run the Cypher query in Neo4j, and use the Cypher query results to respond to the user’s query. You’ll leverage LangChain’s ```GraphCypherQAChain``` for this. \n",
    "\n",
    "If we were to use a normal SQL database we would use langchain's own ```SQLDatabase``` instead ```Neo4jGraph``` & ```SQLDatabaseChain``` instead ```GraphCypherQAChain```\n",
    "\n",
    "### DB Connection neo4j-style\n",
    "To get started we need first to have a LLM capable database connection with ```Neo4jGraph```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=os.getenv(\"NEO4J_URI\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    ")\n",
    "\n",
    "graph.refresh_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neo4jGraph object is a LangChain wrapper that allows LLMs to execute queries on your Neo4j instance. You instantiate graph using your Neo4j credentials, and you call graph.refresh_schema() to sync any recent changes to your instance.\n",
    "\n",
    "### cypher generation template. \n",
    "We want the LLM to generate a Cypher (neo4j query language) query based in a human prompt.\n",
    "\n",
    "This needs heavy prompting engineering and serious trial and error:\n",
    "- we need the answer to provide the query and only the query\n",
    "- It should only query, not insert, update or delete\n",
    "- we rather it use alias to make the query readable for debug purposes\n",
    "- we need to inform beforehand of the equivalent in the DB of certain status, businmess particulars that are not self evident from the schema i.e: \n",
    "    - visit is concluded if has a discharge date\n",
    "    - payment was rejected/pending if it does not have the service_date in the COVERED BY relationship\n",
    "    - Use abbreviations when filtering on hospital states (e.g. \"Texas\" is \"TX\", \"Colorado\" is \"CO\", \"North Carolina\" is \"NC\",...)\n",
    "    - list some categorical values of a few node properties for it to know beforehand\n",
    "    - etc.\n",
    "    \n",
    "All of the detail you provide in your prompt template improves the LLM’s chance of generating a correct Cypher query for a given question. If you’re curious about how necessary all this detail is, try creating your own prompt template with as few details as possible. Then run questions through your Cypher chain and see whether it correctly generates Cypher queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "cypher_generation_template = \"\"\"\n",
    "Task:\n",
    "Generate Cypher query for a Neo4j graph database.\n",
    "\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "Note:\n",
    "Do not include any explanations or apologies in your responses.\n",
    "Do not respond to any questions that might ask anything other than\n",
    "for you to construct a Cypher statement. Do not include any text except\n",
    "the generated Cypher statement. Make sure the direction of the relationship is\n",
    "correct in your queries. Make sure you alias both entities and relationships\n",
    "properly. Do not run any queries that would add to or delete from\n",
    "the database. Make sure to alias all statements that follow as with\n",
    "statement (e.g. WITH v as visit, c.billing_amount as billing_amount)\n",
    "If you need to divide numbers, make sure to\n",
    "filter the denominator to be non zero.\n",
    "\n",
    "Examples:\n",
    "# Who is the oldest patient and how old are they?\n",
    "MATCH (p:Patient)\n",
    "RETURN p.name AS oldest_patient,\n",
    "       duration.between(date(p.dob), date()).years AS age\n",
    "ORDER BY age DESC\n",
    "LIMIT 1\n",
    "\n",
    "# Which physician has billed the least to Cigna\n",
    "MATCH (p:Payer)<-[c:COVERED_BY]-(v:Visit)-[t:TREATS]-(phy:Physician)\n",
    "WHERE p.name = 'Cigna'\n",
    "RETURN phy.name AS physician_name, SUM(c.billing_amount) AS total_billed\n",
    "ORDER BY total_billed\n",
    "LIMIT 1\n",
    "\n",
    "# Which state had the largest percent increase in Cigna visits\n",
    "# from 2022 to 2023?\n",
    "MATCH (h:Hospital)<-[:AT]-(v:Visit)-[:COVERED_BY]->(p:Payer)\n",
    "WHERE p.name = 'Cigna' AND v.admission_date >= '2022-01-01' AND\n",
    "v.admission_date < '2024-01-01'\n",
    "WITH h.state_name AS state, COUNT(v) AS visit_count,\n",
    "     SUM(CASE WHEN v.admission_date >= '2022-01-01' AND\n",
    "     v.admission_date < '2023-01-01' THEN 1 ELSE 0 END) AS count_2022,\n",
    "     SUM(CASE WHEN v.admission_date >= '2023-01-01' AND\n",
    "     v.admission_date < '2024-01-01' THEN 1 ELSE 0 END) AS count_2023\n",
    "WITH state, visit_count, count_2022, count_2023,\n",
    "     (toFloat(count_2023) - toFloat(count_2022)) / toFloat(count_2022) * 100\n",
    "     AS percent_increase\n",
    "RETURN state, percent_increase\n",
    "ORDER BY percent_increase DESC\n",
    "LIMIT 1\n",
    "\n",
    "# How many non-emergency patients in North Carolina have written reviews?\n",
    "MATCH (r:Review)<-[:WRITES]-(v:Visit)-[:AT]->(h:Hospital)\n",
    "WHERE h.state_name = 'NC' and v.admission_type <> 'Emergency'\n",
    "RETURN count(*)\n",
    "\n",
    "String category values:\n",
    "Test results are one of: 'Inconclusive', 'Normal', 'Abnormal'\n",
    "Visit statuses are one of: 'OPEN', 'DISCHARGED'\n",
    "Admission Types are one of: 'Elective', 'Emergency', 'Urgent'\n",
    "Payer names are one of: 'Cigna', 'Blue Cross', 'UnitedHealthcare', 'Medicare',\n",
    "'Aetna'\n",
    "\n",
    "A visit is considered open if its status is 'OPEN' and the discharge date is\n",
    "missing.\n",
    "Use abbreviations when\n",
    "filtering on hospital states (e.g. \"Texas\" is \"TX\",\n",
    "\"Colorado\" is \"CO\", \"North Carolina\" is \"NC\",\n",
    "\"Florida\" is \"FL\", \"Georgia\" is \"GA\", etc.)\n",
    "\n",
    "Make sure to use IS NULL or IS NOT NULL when analyzing missing properties.\n",
    "Never return embedding properties in your queries. You must never include the\n",
    "statement \"GROUP BY\" in your query. Make sure to alias all statements that\n",
    "follow as with statement (e.g. WITH v as visit, c.billing_amount as\n",
    "billing_amount)\n",
    "If you need to divide numbers, make sure to filter the denominator to be non\n",
    "zero.\n",
    "\n",
    "The question is:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "cypher_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=cypher_generation_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "> ⚠️ **Note:** The above prompt template provides the LLM with four examples of valid Cypher queries for your graph. Giving the LLM a few examples and then asking it to perform a task is known as few-shot prompting, and it’s a simple yet powerful technique for improving generation accuracy. However, few-shot prompting might not be sufficient for Cypher query generation, especially if you have a complicated graph. One way to improve this is to create a vector database that embeds example user questions/queries and stores their corresponding Cypher queries as metadata. When a user asks a question, you inject Cypher queries from semantically similar questions into the prompt, providing the LLM with the most relevant examples needed to answer the current question.\n",
    "---\n",
    "\n",
    "### cypher results interpretation template\n",
    "Next, you define the prompt template for the question-answer component of your chain. This template tells the LLM to use the Cypher query results from launching the query obtained with the previous prompt to generate a nicely-formatted answer to the user’s query. Again heavy prompting required, but not quite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_generation_template = \"\"\"You are an assistant that takes the results\n",
    "from a Neo4j Cypher query and forms a human-readable response. The\n",
    "query results section contains the results of a Cypher query that was\n",
    "generated based on a user's natural language question. The provided\n",
    "information is authoritative, you must never doubt it or try to use\n",
    "your internal knowledge to correct it. Make the answer sound like a\n",
    "response to the question.\n",
    "\n",
    "Query Results:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "If the provided information is empty, say you don't know the answer.\n",
    "Empty information looks like this: []\n",
    "\n",
    "If the information is not empty, you must provide an answer using the\n",
    "results. If the question involves a time duration, assume the query\n",
    "results are in units of days unless otherwise specified.\n",
    "\n",
    "When names are provided in the query results, such as hospital names,\n",
    "beware  of any names that have commas or other punctuation in them.\n",
    "For instance, 'Jones, Brown and Murray' is a single hospital name,\n",
    "not multiple hospitals. Make sure you return any list of names in\n",
    "a way that isn't ambiguous and allows someone to tell what the full\n",
    "names are.\n",
    "\n",
    "Never say you don't have the right information if there is data in\n",
    "the query results. Always use the data in the query results.\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "qa_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], template=qa_generation_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GraphCypherQAChain\n",
    "This chain will integrate both parts: the creation of the query and the parse of the results retrieved by launching it.\n",
    "\n",
    "Here’s a breakdown of the parameters used in GraphCypherQAChain.from_llm():\n",
    "\n",
    "- cypher_llm: The LLM used to generate Cypher queries.\n",
    "- qa_llm: The LLM used to generate an answer given Cypher query results.\n",
    "- graph: The Neo4jGraph object that connects to your Neo4j instance. The database connection if you will.\n",
    "- verbose: Whether intermediate steps your chain performs should be printed.\n",
    "- qa_prompt: The prompt template for responding to questions/queries.\n",
    "- cypher_prompt: The prompt template for generating Cypher queries.\n",
    "- validate_cypher: If true, the Cypher query will be inspected for errors and corrected before running. Note that this doesn’t guarantee the Cypher query will be valid. Instead, it corrects simple syntax errors that are easily detectable using regular expressions.\n",
    "- top_k: The number of query results to include in qa_prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "HOSPITAL_QA_MODEL = os.getenv(\"HOSPITAL_QA_MODEL\")\n",
    "HOSPITAL_CYPHER_MODEL = os.getenv(\"HOSPITAL_CYPHER_MODEL\")\n",
    "\n",
    "hospital_cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm=ChatOpenAI(model=HOSPITAL_CYPHER_MODEL, temperature=0),\n",
    "    qa_llm=ChatOpenAI(model=HOSPITAL_QA_MODEL, temperature=0),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    qa_prompt=qa_generation_prompt,\n",
    "    cypher_prompt=cypher_generation_prompt,\n",
    "    validate_cypher=True,\n",
    "    top_k=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (v:Visit)-[:AT]->(h:Hospital)\n",
      "WHERE h.state_name = 'NC' AND v.admission_type = 'Emergency' AND v.status = 'DISCHARGED'\n",
      "WITH v, duration.between(date(v.admission_date), date(v.discharge_date)).days AS visit_duration\n",
      "RETURN AVG(visit_duration) AS average_visit_duration\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'average_visit_duration': 15.072972972972991}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The average visit duration for emergency visits in North Carolina is approximately 15.07 days.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"What is the average visit duration for emergency visits in North Carolina?\"\"\"\n",
    "response = hospital_cypher_chain.invoke(question)\n",
    "response.get(\"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (h:Hospital)<-[:AT]-(v:Visit)-[:COVERED_BY]->(p:Payer)\n",
      "WHERE p.name = 'Medicaid' AND v.admission_date >= '2022-01-01' AND\n",
      "v.admission_date < '2024-01-01'\n",
      "WITH h.state_name AS state, COUNT(v) AS visit_count,\n",
      "     SUM(CASE WHEN v.admission_date >= '2022-01-01' AND\n",
      "     v.admission_date < '2023-01-01' THEN 1 ELSE 0 END) AS count_2022,\n",
      "     SUM(CASE WHEN v.admission_date >= '2023-01-01' AND\n",
      "     v.admission_date < '2024-01-01' THEN 1 ELSE 0 END) AS count_2023\n",
      "WITH state, visit_count, count_2022, count_2023,\n",
      "     (toFloat(count_2023) - toFloat(count_2022)) / toFloat(count_2022) * 100\n",
      "     AS percent_increase\n",
      "RETURN state, percent_increase\n",
      "ORDER BY percent_increase DESC\n",
      "LIMIT 1\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'state': 'TX', 'percent_increase': 8.823529411764707}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The state with the largest percent increase in Medicaid visits from 2022 to 2023 was Texas (TX) with a percent increase of 8.82%.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"Which state had the largest percent increase in Medicaid visits from 2022 to 2023?\"\"\"\n",
    "response = hospital_cypher_chain.invoke(question)\n",
    "response.get(\"result\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
