{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Get Familiar With LangChain\n",
    "You  instantiate a ChatOpenAI model using GPT 3.5 Turbo as the base LLM, and you set temperature to 0. \n",
    "\n",
    "OpenAI offers a diversity of models with varying price points, capabilities, and performances. GPT 3.5 turbo is a great model to start with because it performs well in many use cases and is cheaper than more recent models like GPT 4 and beyond.\n",
    "\n",
    "---\n",
    "> ⚠️ **Note:** It’s a common misconception that setting temperature=0 guarantees deterministic responses from GPT models. While responses are closer to deterministic when temperature=0, there’s no guarantee that you’ll get the same response for identical requests. Because of this, GPT models might output slightly different results than what you see in the examples throughout this tutorial.\n",
    "---\n",
    "\n",
    "## Simple query with system and human messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Medicaid managed care is a system in which states contract with managed care organizations (MCOs) to provide healthcare services to Medicaid beneficiaries. These MCOs are responsible for coordinating and delivering healthcare services to enrollees in exchange for a fixed monthly payment per enrollee. Medicaid managed care aims to improve access to care, enhance quality, and control costs for Medicaid beneficiaries.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "from langchain_intro.chatbot import chat_model\n",
    "\n",
    "messages = [\n",
    "     SystemMessage(\n",
    "         content=\"\"\"You're an assistant knowledgeable about\n",
    "         healthcare. Only answer healthcare-related questions.\"\"\"\n",
    "     ),\n",
    "     HumanMessage(content=\"What is Medicaid managed care?\"),\n",
    " ]\n",
    "chat_model.invoke(messages).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAINS\n",
    "### Prompt templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt=PromptTemplate(input_variables=['context'], template=\"Your job is to use patient\\nreviews to answer questions about their experience at a\\nhospital. Use the following context to answer questions.\\nBe as detailed as possible, but don't make up any information\\nthat's not from the context. If you don't know an answer, say\\nyou don't know.\\n\\nPatient reviews:\\n\\n{context}\\n\")\n",
      "prompt=PromptTemplate(input_variables=['question'], template='{question}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Your job is to use patient\\nreviews to answer questions about their experience at a\\nhospital. Use the following context to answer questions.\\nBe as detailed as possible, but don't make up any information\\nthat's not from the context. If you don't know an answer, say\\nyou don't know.\\n\\nPatient reviews:\\n\\n{context}\\n\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "review_system_template_str = \"\"\"Your job is to use patient\n",
    "reviews to answer questions about their experience at a\n",
    "hospital. Use the following context to answer questions.\n",
    "Be as detailed as possible, but don't make up any information\n",
    "that's not from the context. If you don't know an answer, say\n",
    "you don't know.\n",
    "\n",
    "Patient reviews:\n",
    "\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "review_system_prompt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate.from_template(review_system_template_str)\n",
    ")\n",
    "print (review_system_prompt)\n",
    "\n",
    "review_human_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate.from_template(\"{question}\")\n",
    ")\n",
    "print(review_human_prompt)\n",
    "\n",
    "\n",
    "messages = [review_system_prompt, review_human_prompt]\n",
    "review_chat_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "review_chat_prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains and LangChain Expression Language (LCEL)\n",
    "The glue that connects chat models, prompts, and other objects in LangChain is the chain. A chain is nothing more than a sequence of calls between objects in LangChain. The recommended way to build chains is to use the LangChain Expression Language (LCEL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Yes, the patient had a positive experience during their stay at the hospital.'\n"
     ]
    }
   ],
   "source": [
    "# define the chain\n",
    "review_chain = review_chat_prompt_template | chat_model\n",
    "\n",
    "context = \"I had a great stay!\"\n",
    "question = \"Did the patient have a positive experience?\"\n",
    "# invoke it passing the parameters of the template for the first step\n",
    "print(review_chain.invoke({\"context\": context, \"question\": question}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the LCEL allows you to create arbitrary-length chains with the pipe symbol (|). For instance, if you wanted to format the model’s response, then you could add an output parser to the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the patient had a positive experience during their stay at the hospital.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "review_chain = review_chat_prompt_template | chat_model | output_parser\n",
    "\n",
    "context = \"I had a great stay!\"\n",
    "question = \"Did the patient have a positive experience?\"\n",
    "# invoke it passing the parameters of the template for the first step\n",
    "print(review_chain.invoke({\"context\": context, \"question\": question}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-augmented generation\n",
    "The goal of our review_chain is to answer questions about patient experiences in the hospital from their reviews. So far, we manually passed reviews in as context for the question. While this can work for a small number of reviews, it doesn’t scale well. Moreover, even if you can fit all reviews into the model’s context window, there’s no guarantee it will use the correct reviews when answering a question.\n",
    "\n",
    "To overcome this, you need a retriever. The process of retrieving relevant documents and passing them to a language model to answer questions is known as retrieval-augmented generation (RAG).\n",
    "\n",
    "For this example, you’ll store all the reviews in a vector database called ChromaDB. If you’re unfamiliar with this database tool and topics, then check out [Embeddings and Vector Databases with ChromaDB](https://realpython.com/chromadb-vector-database/) before continuing.\n",
    "\n",
    "In our example we will get the reviews that we have in a csv/\"human format\" and *embed* it on the vector database.\n",
    "This process implies converting the content of the document into a dense numerical representation called a vector. These vectors are lists of numbers and can have hundreds or thousands of dimensions. The main goal of creating embeddings for documents is to represent data like text, images, or audio in a numerical format that computational algorithms can more easily process. It allows you to compare documents to each other based on their content. Its the base of the LLM learning and also for the retrieval augmented generation:  In a RAG pipeline, when a user submits a query, that query is also converted into an embedding vector, and a similarity search is performed in the vector database to find the most relevant document embeddings. The actual documents associated with these relevant embeddings are then retrieved. These retrieved documents serve as context for a language model to help it generate a more accurate and informed response that goes beyond its original training data\n",
    "\n",
    "---\n",
    "> ⚠️ **Note:** In practice, if you’re embedding a large document, you should use a text splitter. Text splitters break the document into smaller chunks before running them through an embedding model. This is important because embedding models have a fixed-size context window, and as the size of the text grows, an embedding’s ability to accurately represent the text decreases. For this example, you can embed each review individually because they’re relatively small.\n",
    "---\n",
    "\n",
    "### feed the vector database\n",
    "- we'll use langchain own CSVLoader\n",
    "- and its vector database Chroma connector\n",
    "- we need an empty folder called chroma_data at the level of the notebook\n",
    "\n",
    "Keep in mind that as many times we run the \"feeder\" we add the data again: *we will duplicate* the review data embedded. Empty the chroma_data folder and restart the jupyter kernel to reload it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "REVIEWS_CSV_PATH = \"data/reviews.csv\"\n",
    "REVIEWS_CHROMA_PATH = \"chroma_data\"\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "loader = CSVLoader(file_path=REVIEWS_CSV_PATH, source_column=\"review\")\n",
    "reviews = loader.load()\n",
    "\n",
    "reviews_vector_db = Chroma.from_documents(\n",
    "    reviews, OpenAIEmbeddings(), persist_directory=REVIEWS_CHROMA_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our relevant data transformed into vectors and stored in chroma, ready for RAG\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a technique designed to enhance the capabilities of Large Language Models (LLMs). It addresses a key limitation of LLMs: their reliance on fixed training datasets, which can result in outdated or incomplete information.\n",
    "RAG works by combining an LLM with an external knowledge base, such as a vector database. When a user submits a query, the RAG system performs the following steps:\n",
    "\n",
    "- The system receives the user query.\n",
    "\n",
    "- The query is converted into a vector representation (embedding).\n",
    "\n",
    "- A similarity search is conducted in the external knowledge base (like a vector database) using the query embedding to find the most relevant information or documents \n",
    "\n",
    "- The most relevant data or documents are retrieved from the database.\n",
    "\n",
    "- This retrieved information is then incorporated into the prompt that is sent to the LLM. This provides crucial context to the model [previous turns, 340, 354].\n",
    "\n",
    "- The LLM processes both the original user query and the retrieved information.\n",
    "\n",
    "- Finally, the LLM generates a response or takes an action based on the provided context and query.\n",
    "\n",
    "\n",
    "RAG significantly enhances LLMs in several ways:\n",
    "\n",
    "- It allows LLMs to access and utilize up-to-date information.\n",
    "\n",
    "- It enables LLMs to provide domain-specific expertise or answer questions about data not included in their original training [previous turns, 71, 340, 356, 358, 490, 489].\n",
    "\n",
    "- It helps reduce hallucination (generating false information) by grounding the LLM's responses in retrieved facts.\n",
    "\n",
    "- It offers a cost-effective alternative to fine-tuning or retraining models for incorporating new knowledge.\n",
    "\n",
    "- It leads to the generation of more accurate, informed, specific, and detailed responses that go beyond the LLM's pre-trained knowledge \n",
    "\n",
    "### Explaining RAG with NotebookLM sources\n",
    "it is widely stated and confirmed that NotebookLM utilizes Retrieval-Augmented Generation (RAG) to manage and interact with the sources you provide.\n",
    "\n",
    "It is a key aspect of NotebookLM because it allows grounding in your data: Unlike general LLMs that might pull information from their vast training data, RAG ensures that NotebookLM's responses are grounded in the specific documents, notes, web pages, or other sources you've uploaded. This dramatically reduces \"hallucinations\" (the AI making up facts) and makes the output more reliable and relevant to your context.\n",
    "\n",
    "How it Works (Simplified):\n",
    "1. Retrieval: When you ask a question or interact with NotebookLM, it first uses an information retrieval system to search through your uploaded sources. It finds the most relevant passages, chunks of text, or even specific quotes.\n",
    "2. Augmentation: These retrieved snippets are then provided as additional context to the underlying Large Language Model (LLM), which is typically Gemini 1.5 Pro in NotebookLM's case.\n",
    "3. Generation: The LLM then uses this specific, retrieved information, along with its general language understanding, to generate a coherent and accurate response.\n",
    "\n",
    "RAG enables NotebookLM to effectively summarize documents, create study guides, identify key themes, and spark new ideas based on your provided material.\n",
    "In essence, NotebookLM is designed as an \"end-user customizable RAG product,\" allowing you to create a personalized AI expert based on your own knowledge base.\n",
    "\n",
    "### semantic search in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'review_id: 73\\nvisit_id: 7696\\nreview: I had a frustrating experience at the hospital. The communication between the medical staff and me was unclear, leading to misunderstandings about my treatment plan. Improvement is needed in this area.\\nphysician_name: Maria Thompson\\nhospital_name: Little-Spencer\\npatient_name: Terri Smith'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "REVIEWS_CHROMA_PATH = \"chroma_data/\"\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# connect to the vector database\n",
    "reviews_vector_db = Chroma(\n",
    "     persist_directory=REVIEWS_CHROMA_PATH,\n",
    "     embedding_function=OpenAIEmbeddings(),\n",
    " )\n",
    "\n",
    "# semantic search in the database itself. we look for similar and the three best (k=3)\n",
    "question = \"\"\"Has anyone complained about communication with the hospital staff?\"\"\"\n",
    "relevant_docs = reviews_vector_db.similarity_search(question, k=3)\n",
    "\n",
    "relevant_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'review_id: 521\\nvisit_id: 631\\nreview: I had a challenging time at the hospital. The medical care was adequate, but the lack of communication between the staff and me left me feeling frustrated and confused about my treatment plan.\\nphysician_name: Samantha Mendez\\nhospital_name: Richardson-Powell\\npatient_name: Kurt Gordon'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs[1].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### passing a vector database retriever to the chain\n",
    "We now use this to pass as many reviews that match/are similar to the criteria we want. We do not do the search ourselves, we pass a retriever to the chain\n",
    "\n",
    "- create a retriever pointed to our vector reviews db\n",
    "- format the chat prompt\n",
    "    - System prompt\n",
    "    - Human prompt\n",
    "- chat model\n",
    "- create the chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the patient review provided (\"I had a great stay!\"), there is no indication of any complaints about communication with the hospital staff. The patient's experience seems to have been positive overall.\n",
      "Yes, several patients have complained about communication with the hospital staff. For example, Terri Smith mentioned that the communication between the medical staff and her was unclear, leading to misunderstandings about her treatment plan. Ryan Jacobs also noted that the lack of communication from the staff created frustration during his stay at the hospital. Kurt Gordon expressed feeling frustrated and confused about his treatment plan due to the lack of communication between the staff and himself. Additionally, Michele Jones mentioned that the lack of communication about changes in her treatment plan created unnecessary stress.\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "REVIEWS_CHROMA_PATH = \"chroma_data/\"\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# retriever\n",
    "\n",
    "reviews_vector_db = Chroma(\n",
    "    persist_directory=REVIEWS_CHROMA_PATH,\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "reviews_retriever  = reviews_vector_db.as_retriever(k=10)\n",
    "\n",
    "# system propmt. It will receive the reviews as {context}\n",
    "review_system_template_str = \"\"\"Your job is to use patient reviews to answer questions about their experience at a hospital. Use the following context to answer questions.\n",
    "Be as detailed as possible, but don't make up any information that's not from the context. If you don't know an answer, say you don't know.\n",
    "Patient reviews:\n",
    "{context}\n",
    "\"\"\"\n",
    "review_system_prompt_template = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate.from_template(review_system_template_str)\n",
    ")\n",
    "\n",
    "# human prompt. It will receive the question as {question}\n",
    "review_human_prompt_template = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate.from_template(\"{question}\")\n",
    ")\n",
    "\n",
    "# compose the chat prompt with human and system\n",
    "messages = [review_system_prompt_template, review_human_prompt_template]\n",
    "review_chat_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# the chat model\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "# the chain. We pass the reviews retriever as context. the question will be passed with the \"runnablePassthrough\" to the next chain without changes\n",
    "normal_review_chain = review_chat_prompt_template | chat_model | StrOutputParser()\n",
    "retriever_review_chain = (\n",
    "    {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}\n",
    "    | review_chat_prompt_template\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"\"\"Has anyone complained about\n",
    "            communication with the hospital staff?\"\"\"\n",
    "print(normal_review_chain.invoke({\"question\": question, \"context\": \"I had a great stay!\"}))\n",
    "print(retriever_review_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the patient reviews provided, it seems that both Dr. Karen Smith from Wallace-Hamilton and Dr. Matthew Marks from Little-Spencer were well-liked by the patients. Dr. Karen Smith was praised for being skilled, while Dr. Matthew Marks was appreciated for his positive attitude and going above and beyond to make the patient feel comfortable and informed about their treatment. Both physicians received positive feedback from the patients they treated.\n"
     ]
    }
   ],
   "source": [
    "print(retriever_review_chain.invoke(\"\"\"Which doctors where more liked?\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "An agent is a system designed to use an LLM as a reasoning engine to parse whats asked of it and determine which actions to take. They facilitate the interaction between language models and various tools, acting as intermediaries, enabling seamless communication and task execution by leveraging the capabilities of language models.\n",
    "A simple one could be one that decide to look into the pacient reviews or current hospital waiting times (via an API call for example) depending on what's asked.\n",
    "### Fake API/Function to return waiting times\n",
    "In get_current_wait_time(), you pass in a hospital name, check if it’s valid, and then generate a random number to simulate a wait time. In reality, this would be some sort of database query or API call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "def get_current_wait_time(hospital: str) -> int | str:\n",
    "    \"\"\"Dummy function to generate fake wait times\"\"\"\n",
    "\n",
    "    if hospital not in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        return f\"Hospital {hospital} does not exist\"\n",
    "\n",
    "    # Simulate API call delay\n",
    "    time.sleep(1)\n",
    "\n",
    "    return random.randint(0, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tools and Hubs\n",
    " A tool is essentially a capability or function that the language model (LLM), acting as the agent's reasoning engine, can decide to use to perform specific actions. Tools enable agents to go beyond just generating text and interact with the external world or perform tasks the LLM cannot do inherently via different apis or hooks. The LLM within the agent uses its reasoning capabilities to determine which tool to use and what input is necessary for that tool. This process is often referred to as tool-calling\n",
    "\n",
    " A tool can:\n",
    "- Calling external APIs.\n",
    "- Querying databases (like vector databases or SQL databases) dynamically.\n",
    "- Interacting with external services (such as a search engine like Tavily, which is shown as a tool example).\n",
    "- Fetching specific data or information (useful for RAG).\n",
    "- Running custom code or logic.\n",
    "- Looking up information or performing calculations.\n",
    "\n",
    "The tool is defined by \n",
    "- a name\n",
    "- the function it calls \n",
    "- a description of what it does so the LLM will know when and how to use it\n",
    "\n",
    "Notice how description gives the agent instructions as to when it should call the tool. This is where good prompt engineering skills are paramount to ensuring the LLM calls the correct tool with the correct inputs.\n",
    "\n",
    "Langchain Hub: hosted in https://github.com/hwchase17/langchain-hub and now in https://smith.langchain.com/hub, this is a collection of all artifacts useful for working with LangChain primitives such as prompts, chains and agents. The goal of this repository is to be a central resource for sharing and discovering high quality prompts, chains and agents that combine together to form complex LLM applications. \n",
    "In our case we will use the [openai-functions-agent](https://smith.langchain.com/hub/hwchase17/openai-functions-agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.agents import (\n",
    "    create_openai_functions_agent,\n",
    "    Tool,\n",
    "    AgentExecutor,\n",
    ")\n",
    "from langchain import hub\n",
    "\n",
    "# we use our retriever_review_chain we defined in the previous cell\n",
    "# for the review related queries\n",
    "retriever_review_chain = (\n",
    "    {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}\n",
    "    | review_chat_prompt_template\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# tool definitions: one to answer review questions and one to get wait times\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Reviews\",\n",
    "        func=retriever_review_chain.invoke,\n",
    "        description=\"\"\"Useful when you need to answer questions about patient reviews or experiences at the hospital.\n",
    "        Not useful for answering questions about specific visit details such as payer, billing, treatment, diagnosis,\n",
    "        chief complaint, hospital, or physician information. \n",
    "        Pass the entire question as input to the tool. For instance, if the question is \"What do patients think about the triage system?\",\n",
    "        the input should be \"What do patients think about the triage system?\"\n",
    "        \"\"\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Waits\",\n",
    "        func=get_current_wait_time,\n",
    "        description=\"\"\"Use when asked about current wait times at a specific hospital. This tool can only get the current\n",
    "        wait time at a hospital and does not have any information about aggregate or historical wait times. This tool returns wait times in\n",
    "        minutes. Do not pass the word \"hospital\" as input, only the hospital name itself. For instance, if the question is\n",
    "        \"What is the wait time at hospital A?\", the input should be \"A\".\n",
    "        \"\"\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "hospital_agent_prompt = hub.pull(\"hwchase17/openai-functions-agent\") # which is simply {\"system\":\"You are a helpful assistant\", \"human\":\"{input}\"}\n",
    "\n",
    "agent_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\",temperature=0)\n",
    "\n",
    "# create the agent with our tool list\n",
    "hospital_agent = create_openai_functions_agent(\n",
    "    llm=agent_chat_model,\n",
    "    prompt=hospital_agent_prompt,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "#This is the runtime for the agent\n",
    "# Setting return_intermediate_steps and verbose to True \n",
    "# will allow you to see the agent’s thought process and the tools it calls.\n",
    "hospital_agent_executor = AgentExecutor(\n",
    "    agent=hospital_agent,\n",
    "    tools=tools,\n",
    "    return_intermediate_steps=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Waits` with `C`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m1284\u001b[0m\u001b[32;1m\u001b[1;3mThe current wait time at hospital C is 1284 minutes.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the current wait time at hospital C?',\n",
       " 'output': 'The current wait time at hospital C is 1284 minutes.',\n",
       " 'intermediate_steps': [(AgentActionMessageLog(tool='Waits', tool_input='C', log='\\nInvoking: `Waits` with `C`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"__arg1\":\"C\"}', 'name': 'Waits'}})]),\n",
       "   1284)]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_agent_executor.invoke(\n",
    "     {\"input\": \"What is the current wait time at hospital C?\"}\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Reviews` with `What have patients said about their comfort at the hospital?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPatients have mentioned both positive and negative aspects of their comfort at the hospital. One patient appreciated the hospital's dedication to patient comfort, mentioning well-designed private rooms and comfortable furnishings that made their recovery more bearable and contributed to an overall positive experience. However, other patients have complained about uncomfortable beds affecting their ability to get a good night's sleep during their stay.\u001b[0m\u001b[32;1m\u001b[1;3mPatients have mentioned both positive and negative aspects of their comfort at the hospital. One patient appreciated the hospital's dedication to patient comfort, mentioning well-designed private rooms and comfortable furnishings that made their recovery more bearable and contributed to an overall positive experience. However, other patients have complained about uncomfortable beds affecting their ability to get a good night's sleep during their stay.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What have patients said about their comfort at the hospital?',\n",
       " 'output': \"Patients have mentioned both positive and negative aspects of their comfort at the hospital. One patient appreciated the hospital's dedication to patient comfort, mentioning well-designed private rooms and comfortable furnishings that made their recovery more bearable and contributed to an overall positive experience. However, other patients have complained about uncomfortable beds affecting their ability to get a good night's sleep during their stay.\",\n",
       " 'intermediate_steps': [(AgentActionMessageLog(tool='Reviews', tool_input='What have patients said about their comfort at the hospital?', log='\\nInvoking: `Reviews` with `What have patients said about their comfort at the hospital?`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"__arg1\":\"What have patients said about their comfort at the hospital?\"}', 'name': 'Reviews'}})]),\n",
       "   \"Patients have mentioned both positive and negative aspects of their comfort at the hospital. One patient appreciated the hospital's dedication to patient comfort, mentioning well-designed private rooms and comfortable furnishings that made their recovery more bearable and contributed to an overall positive experience. However, other patients have complained about uncomfortable beds affecting their ability to get a good night's sleep during their stay.\")]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_agent_executor.invoke(\n",
    "     {\"input\": \"What have patients said about their comfort at the hospital?\"}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
