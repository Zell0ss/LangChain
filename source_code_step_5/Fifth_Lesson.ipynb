{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the LangChain Agent\n",
    "\n",
    "For this, you’ll deploy your chatbot as a FastAPI endpoint and create a Streamlit UI to interact with the endpoint. This notebook is not really meant to be executed we will only show the code here\n",
    "\n",
    "## Serve the Agent With FastAPI\n",
    "FastAPI is a modern, high-performance web framework for building APIs with Python based on standard type hints. It comes with a lot of great features including development speed, runtime speed, and great community support, making it a great choice for serving your chatbot agent.\n",
    "\n",
    "### Pydantic models\n",
    "You’ll serve your agent through a POST request, so the first step is to define what data you expect to get in the request body and what data the request returns. FastAPI does this with Pydantic __models__:\n",
    "- API receive a query (str) as input\n",
    "- API returns\n",
    "    - the input query (str)\n",
    "    - the answer (str)\n",
    "    - the intermediate steps (a list as we saw in previous lesson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class HospitalQueryInput(BaseModel):\n",
    "    text: str\n",
    "\n",
    "class HospitalQueryOutput(BaseModel):\n",
    "    input: str\n",
    "    output: str\n",
    "    intermediate_steps: list[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### async retry decorator\n",
    "One great feature of FastAPI is its asynchronous serving capabilities. Because your agent calls OpenAI models hosted on an external server, there will always be latency while your agent waits for a response. This is a perfect opportunity for you to use asynchronous programming.\n",
    "\n",
    "Instead of waiting for OpenAI to respond to each of your agent’s requests, you can have your agent make multiple requests in a row and store the responses as they’re received. This will save you a lot of time if you have multiple queries you need your agent to respond to.\n",
    "\n",
    "As discussed previously, there can sometimes be intermittent connection issues with Neo4j that are usually resolved by establishing a new connection. Because of this, you’ll want to implement retry logic that works for asynchronous functions. This decorator will retry any function it decorates three times with a delay of 1 second (unless specified otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "## @async_retry\n",
    "def async_retry(max_retries: int=3, delay: int=1):\n",
    "    def decorator(func):\n",
    "        async def wrapper(*args, **kwargs):\n",
    "            for attempt in range(1, max_retries + 1):\n",
    "                try:\n",
    "                    result = await func(*args, **kwargs)\n",
    "                    return result\n",
    "                except Exception as e:\n",
    "                    print(f\"Attempt {attempt} failed: {str(e)}\")\n",
    "                    await asyncio.sleep(delay)\n",
    "\n",
    "            raise ValueError(f\"Failed after {max_retries} attempts\")\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastAPI Application and view Setup\n",
    "\n",
    "Makes no sense for a single endpoint + health with no auth to separate this into app.py, crud.py and views.py and use routers as is usually the case in a fastapi api. \n",
    "\n",
    "In this code we prepare the FastAPI webservice, \n",
    "\n",
    "The code imports several key components:\n",
    "\n",
    "- FastAPI: The web framework for creating the API endpoints\n",
    "- hospital_rag_agent_executor: The core agent that handles hospital-related queries using RAG\n",
    "- Data Models: HospitalQueryInput and HospitalQueryOutput for request/response validation\n",
    "- Retry Utility: the async_retry decorator for handling intermittent failures we just explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from agents.hospital_rag_agent import hospital_rag_agent_executor\n",
    "from models.hospital_rag_query import HospitalQueryInput, HospitalQueryOutput\n",
    "from utils.async_utils import async_retry\n",
    "\n",
    "# app.py\n",
    "app = FastAPI(\n",
    "    title=\"Hospital Chatbot\",\n",
    "    description=\"Endpoints for a hospital system graph RAG chatbot\",\n",
    ")\n",
    "\n",
    "# crud.py\n",
    "@async_retry(max_retries=10, delay=1)\n",
    "async def invoke_agent_with_retry(query: str):\n",
    "    \"\"\"Retry the agent if a tool fails to run.\n",
    "\n",
    "    This can help when there are intermittent connection issues\n",
    "    to external APIs.\n",
    "    \"\"\"\n",
    "    return await hospital_rag_agent_executor.ainvoke({\"input\": query})\n",
    "\n",
    "# views.py\n",
    "@app.get(\"/\")\n",
    "async def get_status():\n",
    "    return {\"status\": \"running\"}\n",
    "\n",
    "@app.post(\"/hospital-rag-agent\")\n",
    "async def query_hospital_agent(query: HospitalQueryInput) -> HospitalQueryOutput:\n",
    "    query_response = await invoke_agent_with_retry(query.text)\n",
    "    query_response[\"intermediate_steps\"] = [\n",
    "        str(s) for s in query_response[\"intermediate_steps\"]\n",
    "    ]\n",
    "\n",
    "    return query_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing FASTAPI\n",
    "\n",
    "### with curl\n",
    "We need to launch the app from the folder its stored -> ./source_code_step_5/chatbot_api/src with ``` uvicorn main:app --host 0.0.0.0 --port 8000 --env-file .env```\n",
    "\n",
    "We need to pass the .env file so it loads the variables. Then we can test it with curls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "> source_code_step_5/chatbot_api/src\n",
    "> curl http://localhost:8000/docs#/\n",
    "> curl -X 'POST' \\\n",
    "  'http://localhost:8000/hospital-rag-agent' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"text\": \"how are you\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### debuging in vscode\n",
    "\n",
    "In vscode we create a FASTAPI launcher for the main.py on /source_code_step_5/chatbot_api/src\n",
    "\n",
    "Once launched you can test the swagger in http://localhost:8000/docs#/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "    \"version\": \"0.2.0\",\n",
    "    \"configurations\": [\n",
    "        {\n",
    "            \"name\": \"Python Debugger: FastAPI on src\",\n",
    "            \"type\": \"debugpy\",\n",
    "            \"request\": \"launch\",\n",
    "            \"module\": \"uvicorn\",\n",
    "            \"args\": [\n",
    "                \"main:app\",\n",
    "                \"--reload\",\n",
    "                \"--host\", \"0.0.0.0\",\n",
    "                \"--port\", \"8000\"\n",
    "            ],\n",
    "            \"cwd\": \"${workspaceFolder}/source_code_step_5/chatbot_api/src\",\n",
    "            \"env\": {\n",
    "                \"PYTHONPATH\": \"${workspaceFolder}/source_code_step_5/chatbot_api/src\"\n",
    "            },\n",
    "            \"console\": \"integratedTerminal\",\n",
    "            \"justMyCode\": false\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## docker deployment\n",
    "### entrypoint for chatbot\n",
    "You’ll serve this API with Docker and you’ll want to define the following entrypoint file to run inside the container. The command \n",
    "\n",
    "```shell\n",
    "uvicorn main:app --host 0.0.0.0 --port 8000 \n",
    "```\n",
    "runs the FastAPI application at port 8000 on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Run any setup steps or pre-processing tasks here\n",
    "echo \"Starting hospital RAG FastAPI service...\"\n",
    "\n",
    "# Start the main application\n",
    "uvicorn main:app --host 0.0.0.0 --port 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dockerfile for the chatbot\n",
    "The Dockerfile must specify what docker image will be run \n",
    "- wich base is used for the container, in our case we will use python:3.11-slim distribution\n",
    "- then copy the contents from chatbot_api/src/ into the /app directory within the container\n",
    "- install the dependencies from pyproject.toml\n",
    "- and run entrypoint.sh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# chatbot_api/Dockerfile\n",
    "\n",
    "FROM python:3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY ./src/ /app\n",
    "\n",
    "COPY ./pyproject.toml /code/pyproject.toml\n",
    "RUN pip install /code/.\n",
    "\n",
    "EXPOSE 8000\n",
    "CMD [\"sh\", \"entrypoint.sh\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrypoint for the ETL Operations\n",
    "We simply run the python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Run any setup steps or pre-processing tasks here\n",
    "echo \"Running ETL to move hospital data from csvs to Neo4j...\"\n",
    "\n",
    "# Run the ETL script\n",
    "python hospital_bulk_csv_write.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dockerfile for the ETL operations\n",
    "This will run the Data pipeline service for Extract, Transform, Load operations\n",
    "- Will base over python:3.11-slim\n",
    "- then copy the contents from hospital_neo4j_ETL/src/ into the /app directory within the container\n",
    "- install the dependencies from pyproject.toml\n",
    "- and run entrypoint.sh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "FROM python:3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY ./src/ /app\n",
    "\n",
    "COPY ./pyproject.toml /code/pyproject.toml\n",
    "RUN pip install /code/.\n",
    "\n",
    "CMD [\"sh\", \"entrypoint.sh\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### docker-compose\n",
    "\n",
    "In Docker Compose, services are the individual application components or *containers* that make up your multi-container application. Each service represents a separate containerized process that performs a specific function. Much like its counterparts in kubernetes, pods/containers. \n",
    "\n",
    "In this case we create the two that we have defined for the ETL operations and the API and establish precedence. We also pass the .env file to each one. \n",
    "- The first one hospital_neo4j_etl: does the loading operations in neo4j we saw in lesson 3\n",
    "- The second one that will not begin till the first run out is the FASTAPI that will be publishing the port 8000 as port 8000\n",
    "\n",
    "As we specify a build clause, this will prompt docker compose to create the two services/containers by using the dockerfile in each context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "services:\n",
    "  hospital_neo4j_etl:\n",
    "    build:\n",
    "      context: ./hospital_neo4j_etl\n",
    "    env_file:\n",
    "      - .env\n",
    "\n",
    "  chatbot_api:\n",
    "    build:\n",
    "      context: ./chatbot_api\n",
    "    env_file:\n",
    "      - .env\n",
    "    depends_on:\n",
    "      - hospital_neo4j_etl\n",
    "    ports:\n",
    "      - \"8000:8000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the API, along with the ETL you build earlier, open a terminal on the fifth lesson folder /source_code_step_5 and run:\n",
    "```bash\n",
    "$  docker-compose up --build\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
